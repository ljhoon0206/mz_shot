import time
import cv2
import numpy as np
import streamlit as st
import mediapipe as mp
import av
import queue
import base64
import streamlit.components.v1 as components

from streamlit_webrtc import (
    webrtc_streamer,
    VideoProcessorBase,
    RTCConfiguration,
    WebRtcMode,
)

# Mediapipe Face Detection ì´ˆê¸°í™” (ì „ì—­ìœ¼ë¡œ ë‘ë©´ ì„±ëŠ¥ ì´ìŠˆê°€ ìˆì„ ìˆ˜ ìˆì–´, Processor ë‚´ë¶€ì— ë‘ )
mp_face = mp.solutions.face_detection


# ---------------- ì…”í„° ì†Œë¦¬ìš© HTML ìƒì„± í•¨ìˆ˜ ----------------
# * ì´ í•¨ìˆ˜ëŠ” ë³€ê²½ ì—†ì´ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë©°, Base64 ì¸ì½”ë”©ì„ í†µí•´ ì†Œë¦¬ íŒŒì¼ì„ ì›¹ì— í¬í•¨í•©ë‹ˆë‹¤.
def load_shutter_html():
    """
    shutter.wav íŒŒì¼ì„ base64ë¡œ ì½ì–´ì„œ <audio> ìë™ ì¬ìƒ HTMLì„ ë§Œë“¤ì–´ ì¤Œ.
    shutter.wavëŠ” ì´ íŒŒì´ì¬ íŒŒì¼ê³¼ ê°™ì€ í´ë”ì— ìˆì–´ì•¼ í•¨.
    """
    try:
        with open("shutter.wav", "rb") as f:
            audio_bytes = f.read()
        audio_b64 = base64.b64encode(audio_bytes).decode("utf-8")

        html = f"""
        <audio autoplay>
            <source src="data:audio/wav;base64,{audio_b64}" type="audio/wav" />
            ë¸Œë¼ìš°ì €ê°€ audio íƒœê·¸ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        </audio>
        """
        return html
    except FileNotFoundError:
        st.error("shutter.wav íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì´ ìŠ¤í¬ë¦½íŠ¸ì™€ ê°™ì€ í´ë”ì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return ""


def get_face_roll_angle(img_bgr):
    """BGR ì´ë¯¸ì§€ì—ì„œ ì²« ë²ˆì§¸ ì–¼êµ´ì˜ roll angle(ê¸°ìš¸ê¸°) ê³„ì‚°."""
    h, w, _ = img_bgr.shape

    with mp_face.FaceDetection(
        model_selection=0,
        min_detection_confidence=0.6
    ) as face_detector:
        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
        results = face_detector.process(img_rgb)

        if not results.detections:
            return None, None

        detection = results.detections[0]
        keypoints = detection.location_data.relative_keypoints

        right_eye = keypoints[0]
        left_eye = keypoints[1]

        x1, y1 = right_eye.x * w, right_eye.y * h
        x2, y2 = left_eye.x * w, left_eye.y * h

        dx = x2 - x1
        dy = y2 - y1

        angle_rad = np.arctan2(dy, dx)
        angle_deg = np.degrees(angle_rad)

        right_eye_pt = (int(x1), int(y1))
        left_eye_pt = (int(x2), int(y2))

        return angle_deg, (right_eye_pt, left_eye_pt)


def draw_angle_overlay(img_bgr, angle_deg, eye_pts, label=""):
    """íƒ€ê²Ÿ ì´ë¯¸ì§€ ìœ„ì— ëˆˆ ì„  + ê°ë„ í…ìŠ¤íŠ¸ í‘œì‹œ."""
    img = img_bgr.copy()
    if angle_deg is not None and eye_pts is not None:
        (re, le) = eye_pts
        cv2.line(img, re, le, (0, 255, 0), 2)
        text = f"{label} roll: {angle_deg:.1f} deg"
    else:
        text = f"{label} No face"

    cv2.putText(
        img,
        text,
        (30, 40),
        cv2.FONT_HERSHEY_SIMPLEX,
        1.0,
        (0, 0, 255),
        2,
        cv2.LINE_AA,
    )
    return img


class PoseMatchProcessor(VideoProcessorBase):
    """
    ì›¹ìº  í”„ë ˆì„ì„ ë°›ì•„ì„œ:
      - ì—¬ëŸ¬ ì‚¬ëŒì˜ ì–¼êµ´ roll angle ê³„ì‚°
      - íƒ€ê²Ÿ ê°ë„ì™€ ë¹„êµí•´ ì‚¬ëŒë³„ ìœ ì‚¬ë„ ê³„ì‚°
      - ì‚¬ëŒë³„ ìœ ì‚¬ë„ ë°” í‘œì‹œ
      - ì¡°ê±´ ë§Œì¡± ì‹œ ìë™ ìº¡ì²˜ (ìµœê·¼ 10ì¥ ì €ì¥, ìº¡ì²˜ ì‚¬ì§„ì—ëŠ” UI ì—†ìŒ)
      - ìº¡ì²˜ ìˆœê°„ ì…”í„° ì†Œë¦¬ ì‹ í˜¸(shutter_queue) ë³´ëƒ„
    """

    def __init__(self):
        # streamlit-webrtc í”„ë ˆì„ í¬ë§· ê³ ì • (artifact ì¤„ì´ê¸°ìš©)
        self._frame_format = "bgr24"

        # íƒ€ê²Ÿ ê°ë„ & ì¡°ê±´
        self.ref_angle = None
        self.tolerance = 5.0          # í—ˆìš© ê°ë„ ì°¨
        self.cooldown_sec = 3.0       # ìº¡ì²˜ ì¿¨ë‹¤ìš´
        self.last_capture_time = 0.0

        # í˜„ì¬ í”„ë ˆì„ ê¸°ì¤€ ì‚¬ëŒë³„ ì •ë³´
        # [{"id":1, "angle":..., "sim":...}, ...]
        self.person_infos = []

        # ìë™ ìº¡ì²˜ëœ ì´ë¯¸ì§€ë“¤ (UI ì—†ëŠ” ì›ë³¸)
        self.captured_images = []

        # FaceDetectionì€ í•œ ë²ˆë§Œ ìƒì„± (ì„±ëŠ¥)
        self.face_detector = mp_face.FaceDetection(
            model_selection=0,
            min_detection_confidence=0.6
        )

        # ğŸ”” ì…”í„° ì†Œë¦¬ íŠ¸ë¦¬ê±°ìš© í (<- ì´ ë¶€ë¶„ì´ ë‘ ë²ˆì§¸ ì½”ë“œì—ì„œ ê°€ì ¸ì˜¨ í•µì‹¬ì…ë‹ˆë‹¤)
        self.shutter_queue = queue.Queue()

    def recv(self, frame):
        # í”„ë ˆì„ì„ bgr24ë¡œ ë³€í™˜ (í¬ë§· ê³ ì •)
        img = frame.to_ndarray(format="bgr24")

        # ğŸ”¹ UIê°€ ì—†ëŠ” ì›ë³¸ í”„ë ˆì„ (ìº¡ì²˜ìš©)
        raw_img = img.copy()

        h, w, _ = img.shape

        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        results = self.face_detector.process(img_rgb)

        self.person_infos = []
        faces_for_capture = []

        if results and results.detections:
            # 1) ê° detectionì— ëŒ€í•´ ê°ë„/ìœ ì‚¬ë„/ì¤‘ì‹¬ xì¢Œí‘œ ê³„ì‚°
            temp_list = []
            for det in results.detections:
                keypoints = det.location_data.relative_keypoints
                right_eye = keypoints[0]
                left_eye = keypoints[1]

                x1, y1 = right_eye.x * w, right_eye.y * h
                x2, y2 = left_eye.x * w, left_eye.y * h
                dx, dy = x2 - x1, y2 - y1

                angle_rad = np.arctan2(dy, dx)
                angle_deg = np.degrees(angle_rad)

                sim = None
                if self.ref_angle is not None:
                    diff = abs(angle_deg - self.ref_angle)
                    if diff >= self.tolerance:
                        sim = 0.0
                    else:
                        sim = max(0.0, 100.0 * (1.0 - diff / self.tolerance))

                center_x = (x1 + x2) / 2.0
                temp_list.append(
                    {
                        "det": det,
                        "angle": angle_deg,
                        "sim": sim,
                        "center_x": center_x,
                    }
                )

            # 2) ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ ìˆœìœ¼ë¡œ ì •ë ¬ â†’ P1, P2, ...
            temp_list.sort(key=lambda d: d["center_x"])

            # 3) ê·¸ë¦¬ê¸° + ìƒíƒœ ì €ì¥ (ì´í•˜ ì›ë³¸ ì½”ë“œì™€ ë™ì¼)
            for idx, info in enumerate(temp_list, start=1):
                det = info["det"]
                angle_deg = info["angle"]
                sim = info["sim"]

                # Streamlitì— ë³´ì—¬ì¤„ ë°ì´í„°
                self.person_infos.append(
                    {
                        "id": idx,
                        "angle": angle_deg,
                        "sim": sim,
                    }
                )

                # ëˆˆ ì¢Œí‘œ
                keypoints = det.location_data.relative_keypoints
                right_eye = keypoints[0]
                left_eye = keypoints[1]
                x1, y1 = right_eye.x * w, right_eye.y * h
                x2, y2 = left_eye.x * w, left_eye.y * h
                right_eye_pt = (int(x1), int(y1))
                left_eye_pt = (int(x2), int(y2))

                # ëˆˆ ì„ 
                cv2.line(img, right_eye_pt, left_eye_pt, (0, 255, 0), 2)

                # ì–¼êµ´ ë°•ìŠ¤
                rel_box = det.location_data.relative_bounding_box
                bx = int(rel_box.xmin * w)
                by = int(rel_box.ymin * h)
                bw = int(rel_box.width * w)
                bh = int(rel_box.height * h)
                bx = max(0, bx)
                by = max(0, by)
                bw = max(0, bw)
                bh = max(0, bh)
                cv2.rectangle(img, (bx, by), (bx + bw, by + bh), (255, 255, 0), 1)

                # ì‚¬ëŒ ë²ˆí˜¸ + ê°ë„ + ìœ ì‚¬ë„ í…ìŠ¤íŠ¸
                if sim is not None:
                    text = f"P{idx} angle:{angle_deg:.1f} deg | sim:{sim:.0f}%"
                else:
                    text = f"P{idx} angle:{angle_deg:.1f} deg"
                cv2.putText(
                    img,
                    text,
                    (bx, max(0, by - 10)),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.5,
                    (0, 255, 255),
                    1,
                    cv2.LINE_AA,
                )

                # ì–¼êµ´ ì•„ë˜ ê°œì¸ ìœ ì‚¬ë„ ë°”
                if sim is not None:
                    bar_x1 = bx
                    bar_y1 = by + bh + 10
                    bar_x2 = bx + bw
                    bar_y2 = bar_y1 + 10

                    # í™”ë©´ ë°–ìœ¼ë¡œ ì•ˆ ë‚˜ê°€ê²Œ í´ë¨í•‘
                    bar_x1 = max(0, min(bar_x1, w - 1))
                    bar_x2 = max(0, min(bar_x2, w - 1))
                    bar_y1 = max(0, min(bar_y1, h - 1))
                    bar_y2 = max(0, min(bar_y2, h - 1))

                    if bar_x2 > bar_x1 and bar_y2 > bar_y1:
                        cv2.rectangle(
                            img,
                            (bar_x1, bar_y1),
                            (bar_x2, bar_y2),
                            (80, 80, 80),
                            1,
                        )
                        ratio = max(0.0, min(1.0, sim / 100.0))
                        fill_x2 = bar_x1 + int((bar_x2 - bar_x1) * ratio)
                        cv2.rectangle(
                            img,
                            (bar_x1, bar_y1),
                            (fill_x2, bar_y2),
                            (0, 200, 0),
                            -1,
                        )

                # ìº¡ì²˜ í›„ë³´
                if sim is not None:
                    faces_for_capture.append(sim)

            # 4) ì—¬ëŸ¬ ëª… ì¤‘ í•˜ë‚˜ë¼ë„ ìœ ì‚¬ë„ ê¸°ì¤€ ë„˜ìœ¼ë©´ ìë™ ìº¡ì²˜
            if self.ref_angle is not None and faces_for_capture:
                max_sim = max(faces_for_capture)
                now = time.time()
                if max_sim >= 90.0 and now - self.last_capture_time > self.cooldown_sec:
                    self.last_capture_time = now

                    # âœ… UI ì—†ëŠ” ì›ë³¸(raw_img)ì„ ì €ì¥
                    self.captured_images.append(raw_img.copy())
                    if len(self.captured_images) > 10:
                        self.captured_images.pop(0)

                    # ğŸ”” ì…”í„° ì†Œë¦¬ ì‹ í˜¸ ë³´ë‚´ê¸° (<- ì´ ë¶€ë¶„ì´ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤.)
                    try:
                        self.shutter_queue.put_nowait(True)
                    except queue.Full:
                        pass

                    # í™”ë©´ì—ë§Œ CAPTURED! í…ìŠ¤íŠ¸ í‘œì‹œ
                    cv2.putText(
                        img,
                        "CAPTURED!",
                        (30, h - 40),
                        cv2.FONT_HERSHEY_SIMPLEX,
                        1.2,
                        (0, 0, 255),
                        3,
                        cv2.LINE_AA,
                    )

            top_text = f"Faces: {len(temp_list)}"
            if self.ref_angle is not None:
                top_text += f" | target:{self.ref_angle:.1f} deg"
        else:
            top_text = "No face"
            self.person_infos = []

        # ìƒë‹¨ ê³µí†µ í…ìŠ¤íŠ¸
        cv2.putText(
            img,
            top_text,
            (30, 40),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.8,
            (0, 255, 255),
            2,
            cv2.LINE_AA,
        )

        return av.VideoFrame.from_ndarray(img, format="bgr24")


def main():
    st.title("íƒ€ê²Ÿ í¬ì¦ˆ ìœ ì‚¬ë„ ê¸°ë°˜ ìë™ ì´¬ì˜ (ì—¬ëŸ¬ ëª… + ì „ë©´/í›„ë©´ ì§€ì›)")

    st.markdown(
        """
        1. **íƒ€ê²Ÿ ì‚¬ì§„**ì„ ì—…ë¡œë“œí•˜ë©´ ì–¼êµ´ ê¸°ìš¸ê¸°(roll angle)ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.  
        2. ì›¹ìº ì„ ì¼œë©´ ì‹¤ì‹œê°„ìœ¼ë¡œ **í™”ë©´ ì† ì—¬ëŸ¬ ì‚¬ëŒ**ì˜ ê°ë„ë¥¼ ê°ê° ê³„ì‚°í•´ì„œ,  
           **ê°ë„ ì°¨ì´ê°€ ì„¤ì •ê°’ ì´í•˜**ê°€ ë˜ë©´ ìë™ìœ¼ë¡œ ì‚¬ì§„ì„ ìº¡ì²˜í•©ë‹ˆë‹¤.  
        3. ì‚¬ëŒì€ ì´ë¯¸ì§€ **ì™¼ìª½ì— ìˆëŠ” ì‚¬ëŒë¶€í„° P1, P2, ...** ìˆœì„œë¡œ ë²ˆí˜¸ê°€ ë¶™ê³ ,  
           ê° ì‚¬ëŒ ì•„ë˜ì— **ê°œë³„ ìœ ì‚¬ë„ ë°”**ê°€ í‘œì‹œë©ë‹ˆë‹¤.  
        4. ëª¨ë°”ì¼ì—ì„œ ì „ë©´/í›„ë©´ ì¹´ë©”ë¼ë¥¼ ì„ íƒí•´ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
        5. ìº¡ì²˜ëœ ê²°ê³¼ë¬¼ì—ëŠ” **UIê°€ ì „í˜€ ì—†ëŠ” ê¹¨ë—í•œ ì‚¬ì§„ë§Œ** ë‚¨ìŠµë‹ˆë‹¤.
        """
    )

    # ğŸ”” ì…”í„° ì†Œë¦¬ HTML ë¯¸ë¦¬ ì¤€ë¹„
    SHUTTER_HTML = load_shutter_html()

    # --- íƒ€ê²Ÿ ì‚¬ì§„ ì—…ë¡œë“œ ---
    st.sidebar.header("â‘  íƒ€ê²Ÿ ì‚¬ì§„ ì—…ë¡œë“œ")
    ref_file = st.sidebar.file_uploader(
        "íƒ€ê²Ÿ í¬ì¦ˆ ì‚¬ì§„ (jpg, png)",
        type=["jpg", "jpeg", "png"],
        key="ref_upload",
    )

    ref_angle = None
    ref_disp = None

    if ref_file is not None:
        data = ref_file.read()
        arr = np.frombuffer(data, np.uint8)
        ref_img = cv2.imdecode(arr, cv2.IMREAD_COLOR)

        if ref_img is None:
            st.sidebar.error("íƒ€ê²Ÿ ì‚¬ì§„ì„ ì½ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        else:
            ref_angle, eye_pts = get_face_roll_angle(ref_img)
            if ref_angle is None:
                st.sidebar.error("íƒ€ê²Ÿ ì‚¬ì§„ì—ì„œ ì–¼êµ´ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            else:
                st.sidebar.success(f"íƒ€ê²Ÿ ì–¼êµ´ ê°ë„: {ref_angle:.1f}Â°")
                ref_disp = draw_angle_overlay(ref_img, ref_angle, eye_pts, label="target")

    # --- ì´¬ì˜ ì¡°ê±´ ---
    st.sidebar.header("â‘¡ ì´¬ì˜ ì¡°ê±´")
    tolerance = st.sidebar.slider(
        "í—ˆìš© ê°ë„ ì°¨ (deg)",
        min_value=2.0,
        max_value=30.0,
        value=8.0,
        step=1.0,
    )
    cooldown_sec = st.sidebar.slider(
        "ì´¬ì˜ ê°„ ìµœì†Œ ê°„ê²© (ì´ˆ)",
        min_value=0.0,
        max_value=10.0,
        value=3.0,
        step=1.0,
    )

    if ref_disp is not None:
        st.subheader("íƒ€ê²Ÿ ì‚¬ì§„ (ê°ë„ í‘œì‹œ)")
        st.image(ref_disp, channels="BGR")

    rtc_config = RTCConfiguration(
        {"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
    )

    st.subheader("ì›¹ìº ")

    # ì „ë©´ / í›„ë©´ ì¹´ë©”ë¼ ì„ íƒ (ëª¨ë°”ì¼ì—ì„œ ìœ íš¨)
    cam_mode = st.radio(
        "ì¹´ë©”ë¼ ì„ íƒ",
        ["ì „ë©´", "í›„ë©´"],
        horizontal=True,
    )

    # í•´ìƒë„ëŠ” ì¡°ê¸ˆ ë‚®ê²Œ (480x360) â†’ ì „ì†¡/ë””ì½”ë”© ì•ˆì •ì„± â†‘
    base_constraints = {
        "width": {"ideal": 480},
        "height": {"ideal": 360},
        "frameRate": {"ideal": 15},
    }

    if cam_mode == "ì „ë©´":
        video_constraints = {
            **base_constraints,
            "facingMode": {"ideal": "user"},
        }
    else:  # í›„ë©´
        video_constraints = {
            **base_constraints,
            "facingMode": {"ideal": "environment"},
        }

    webrtc_ctx = webrtc_streamer(
        key="pose-match-capture-multi",
        mode=WebRtcMode.SENDRECV,
        rtc_configuration=rtc_config,
        media_stream_constraints={
            "video": video_constraints,
            "audio": False,
        },
        video_processor_factory=PoseMatchProcessor,
        async_processing=True,
    )

    if webrtc_ctx.video_processor:
        vp: PoseMatchProcessor = webrtc_ctx.video_processor

        # íƒ€ê²Ÿ ê°ë„/ì„¤ì • ì „ë‹¬
        vp.ref_angle = ref_angle
        vp.tolerance = tolerance
        vp.cooldown_sec = cooldown_sec

        # ğŸ”” ì…”í„° í í™•ì¸ â†’ ì‹ í˜¸ ìˆìœ¼ë©´ ì†Œë¦¬ ì¬ìƒ (<- ì´ ë¶€ë¶„ì´ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤.)
        try:
            # Queueì—ì„œ ì‹ í˜¸ë¥¼ êº¼ëƒ…ë‹ˆë‹¤. (Non-blocking)
            if vp.shutter_queue.get_nowait():
                # ì‹ í˜¸ê°€ ìˆìœ¼ë©´ Base64 ì¸ì½”ë”©ëœ ì˜¤ë””ì˜¤ HTMLì„ ì‚½ì…í•˜ì—¬ ì†Œë¦¬ ì¬ìƒ
                components.html(SHUTTER_HTML, height=0)
        except queue.Empty:
            # íê°€ ë¹„ì–´ìˆëŠ” ê²ƒì€ ì •ìƒì…ë‹ˆë‹¤.
            pass
        except Exception:
            # ê¸°íƒ€ ì˜ˆì™¸ ì²˜ë¦¬
            pass

        st.subheader("í˜„ì¬ ìƒíƒœ (ì™¼ìª½ë¶€í„° P1, P2, ...)")

        if ref_angle is None:
            st.warning("íƒ€ê²Ÿ ì‚¬ì§„ì„ ë¨¼ì € ì—…ë¡œë“œí•´ì•¼ ìœ ì‚¬ë„ ê³„ì‚°ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        else:
            if not vp.person_infos:
                st.write("í˜„ì¬ ì–¼êµ´ì„ ì°¾ëŠ” ì¤‘ì…ë‹ˆë‹¤.")
            else:
                for info in vp.person_infos:
                    pid = info["id"]
                    angle = info["angle"]
                    sim = info["sim"]
                    if sim is None:
                        st.write(f"ì‚¬ëŒ P{pid}: ê°ë„ **{angle:.1f}Â°** (íƒ€ê²Ÿ ì—†ìŒ)")
                    else:
                        st.write(
                            f"ì‚¬ëŒ P{pid}: ê°ë„ **{angle:.1f}Â°**, "
                            f"ìœ ì‚¬ë„(ê°ë„ ê¸°ì¤€): **{sim:.0f}%**"
                        )

        st.subheader("ìë™ ì´¬ì˜ëœ ì‚¬ì§„ë“¤")
        # ìƒˆë¡œê³ ì¹¨ ë²„íŠ¼ì€ Streamlitì˜ Rerunì„ ìœ ë°œí•˜ì—¬ ìƒíƒœë¥¼ ê°±ì‹ í•©ë‹ˆë‹¤.
        if st.button("ìº¡ì²˜ ëª©ë¡ ìƒˆë¡œê³ ì¹¨"):
            st.rerun()

        if vp.captured_images:
            for idx, img in enumerate(reversed(vp.captured_images), start=1):
                st.image(img, channels="BGR", caption=f"ìº¡ì²˜ #{idx}")

                success, buf = cv2.imencode(".jpg", img)
                if success:
                    st.download_button(
                        label=f"ì´ ì‚¬ì§„ ë‹¤ìš´ë¡œë“œ #{idx}",
                        data=buf.tobytes(),
                        file_name=f"capture_{idx}.jpg",
                        mime="image/jpeg",
                        key=f"download_{idx}",
                    )
        else:
            st.write("ì•„ì§ ìº¡ì²˜ëœ ì‚¬ì§„ì´ ì—†ìŠµë‹ˆë‹¤. íƒ€ê²Ÿ ê°ë„ì™€ ë¹„ìŠ·í•˜ê²Œ ë§ì¶°ë³´ì„¸ìš”.")


if __name__ == "__main__":
    main()