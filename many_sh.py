import streamlit as st
import cv2
import mediapipe as mp
import time
import numpy as np
import av
import queue
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase, RTCConfiguration
import streamlit.components.v1 as components
import base64

# ---------------- Mediapipe ì´ˆê¸°í™” ----------------
mp_face = mp.solutions.face_detection
mp_hands = mp.solutions.hands
mp_draw = mp.solutions.drawing_utils

FACE_DETECTOR = mp_face.FaceDetection(model_selection=0, min_detection_confidence=0.4)
HAND_DETECTOR = mp_hands.Hands(max_num_hands=2, min_detection_confidence=0.5)

# ---------------- ì›¹ ì•± ì „ì—­ ë³€ìˆ˜ ì„¤ì • ----------------
TARGET_A_MIN, TARGET_A_MAX = 43, 47
TARGET_B_MIN, TARGET_B_MAX = 12, 15
COUNTDOWN_TIME = 3.0  # ì¹´ìš´íŠ¸ë‹¤ìš´ ì‹œê°„ ì„¤ì •

# STUN ì„œë²„ (ê²€ì€ í™”ë©´ ë°©ì§€)
RTC_CONFIGURATION = RTCConfiguration(
    {"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
)


# ---------------- Victory ì œìŠ¤ì²˜ íŒë‹¨ ----------------
def is_victory(lms):
    """ê²€ì§€+ì¤‘ì§€ í´ì§, ì•½ì§€+ìƒˆë¼ ì ‘í˜ì´ë©´ V ì‚¬ì¸ True"""
    try:
        # ì†ê°€ë½ ë(tip)ì´ ë§ˆë””(knuckle)ë³´ë‹¤ ìœ„(yì¢Œí‘œ ì‘ìŒ)ì— ìˆìœ¼ë©´ í´ì§
        return (
                lms.landmark[8].y < lms.landmark[5].y and
                lms.landmark[12].y < lms.landmark[9].y and
                lms.landmark[16].y > lms.landmark[13].y and
                lms.landmark[20].y > lms.landmark[17].y
        )
    except Exception:
        return False


# ---------------- ë¹„ìœ¨ ê³„ì‚° í•¨ìˆ˜ ----------------
def get_face_distances(detection, img_h):
    keypoints = detection.location_data.relative_keypoints
    bbox_h = detection.location_data.relative_bounding_box.height

    if bbox_h == 0:
        return {'eye_mouth_ratio': 0.0, 'nose_mouth_ratio': 0.0}

    y_eye_r = keypoints[1].y
    y_eye_l = keypoints[0].y
    y_eye_center = (y_eye_r + y_eye_l) / 2
    y_mouth = keypoints[3].y
    y_nose = keypoints[2].y

    distance_eye_mouth_norm = abs(y_mouth - y_eye_center)
    eye_mouth_ratio = distance_eye_mouth_norm / bbox_h

    distance_nose_mouth_norm = abs(y_mouth - y_nose)
    nose_mouth_ratio = distance_nose_mouth_norm / bbox_h

    return {
        'eye_mouth_ratio': eye_mouth_ratio,
        'nose_mouth_ratio': nose_mouth_ratio
    }


# ---------------- ê²Œì´ì§€ ê·¸ë¦¬ê¸° í•¨ìˆ˜ ----------------
def draw_gauge(img, ratio_percent, x_offset, target_min, target_max, label):
    """í™”ë©´ ì™¼ìª½ì— ìˆ˜ì§ ê²Œì´ì§€ë¥¼ ê·¸ë¦½ë‹ˆë‹¤."""
    gauge_x, gauge_y = 50 + x_offset, 80
    gauge_w, gauge_h = 20, 200

    ratio_percent_clamped = max(0, min(100, ratio_percent))
    is_target = target_min <= ratio_percent_clamped <= target_max

    target_color = (0, 255, 0)
    base_color = (255, 255, 255)
    fill_color = target_color if is_target else (0, 0, 255)

    cv2.rectangle(img, (gauge_x, gauge_y), (gauge_x + gauge_w, gauge_y + gauge_h), base_color, 2)

    fill_height = int(gauge_h * (ratio_percent_clamped / 100))
    fill_y_start = gauge_y + gauge_h - fill_height
    cv2.rectangle(img, (gauge_x, fill_y_start), (gauge_x + gauge_w, gauge_y + gauge_h), fill_color, cv2.FILLED)

    y_max = gauge_y + gauge_h - int(gauge_h * (target_min / 100))
    y_min = gauge_y + gauge_h - int(gauge_h * (target_max / 100))

    cv2.rectangle(img, (gauge_x, y_min), (gauge_x + gauge_w, y_max), (0, 255, 255), 1)

    cv2.putText(img, label, (gauge_x - 10, gauge_y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, base_color, 1)
    cv2.putText(img, f"{ratio_percent_clamped}%", (gauge_x - 10, gauge_y + gauge_h + 30),
                cv2.FONT_HERSHEY_SIMPLEX, 0.6, base_color, 2)

    return is_target, ratio_percent_clamped


# ---------------- ì…”í„° ì†Œë¦¬ìš© HTML ìƒì„± í•¨ìˆ˜ ----------------
def load_shutter_html():
    """
    shutter.wav íŒŒì¼ì„ base64ë¡œ ì½ì–´ì„œ <audio> ìë™ ì¬ìƒ HTMLì„ ë§Œë“¤ì–´ ì¤Œ.
    shutter.wavëŠ” ì´ íŒŒì´ì¬ íŒŒì¼ê³¼ ê°™ì€ í´ë”ì— ìˆì–´ì•¼ í•¨.
    """
    try:
        with open("shutter.wav", "rb") as f:
            audio_bytes = f.read()
        audio_b64 = base64.b64encode(audio_bytes).decode("utf-8")

        html = f"""
        <audio autoplay>
            <source src="data:audio/wav;base64,{audio_b64}" type="audio/wav" />
            ë¸Œë¼ìš°ì €ê°€ audio íƒœê·¸ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        </audio>
        """
        return html
    except FileNotFoundError:
        st.error("shutter.wav íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì…”í„° ì†Œë¦¬ ì¬ìƒì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")
        return ""


# ---------------- VideoProcessor í´ë˜ìŠ¤ (í•µì‹¬) ----------------
class VideoProcessor(VideoTransformerBase):
    def __init__(self):
        # âœ… ì´ë¯¸ì§€ ìº¡ì²˜ ê²°ê³¼ë¥¼ ë©”ì¸ ìŠ¤ë ˆë“œë¡œ ì „ë‹¬í•˜ëŠ” í
        self.result_queue = queue.Queue()
        # âœ… ì…”í„° ì†Œë¦¬ ì¬ìƒ ì‹ í˜¸ë¥¼ ë©”ì¸ ìŠ¤ë ˆë“œë¡œ ì „ë‹¬í•˜ëŠ” í
        self.shutter_queue = queue.Queue()

        self.captured = False
        self.last_capture_time = 0
        self.countdown_active = False
        self.countdown_start_time = 0
        self.face_detector = FACE_DETECTOR
        self.hand_detector = HAND_DETECTOR

    def recv(self, frame: av.VideoFrame) -> av.VideoFrame:
        img = frame.to_ndarray(format="bgr24")
        img_h, img_w, _ = img.shape
        rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        if self.captured:
            cv2.putText(img, "CAPTURED! (Hold)", (img_w // 2 - 150, img_h // 2 + 50),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 3)
            return av.VideoFrame.from_ndarray(img, format="bgr24")

        # ---------------- 1. ì–¼êµ´/ì† ì¸ì‹ ë° ë¹„ìœ¨ í™•ì¸ ----------------
        face_res = self.face_detector.process(rgb)
        face_detected = False
        ratio_ok_A, ratio_ok_B = False, False
        ratio_A_percent, ratio_B_percent = 0, 0
        victory_detected = False

        img_out = img.copy()

        if face_res.detections:
            face_detected = True
            d = face_res.detections[0]
            current_ratios = get_face_distances(d, img_h)
            ratio_A_percent = int(current_ratios['eye_mouth_ratio'] * 100)
            ratio_B_percent = int(current_ratios['nose_mouth_ratio'] * 100)
            ratio_ok_A = TARGET_A_MIN <= ratio_A_percent <= TARGET_A_MAX
            ratio_ok_B = TARGET_B_MIN <= ratio_B_percent <= TARGET_B_MAX
            mp_draw.draw_detection(img_out, d)

        hand_res = self.hand_detector.process(rgb)
        if hand_res.multi_hand_landmarks:
            for handLms in hand_res.multi_hand_landmarks:
                if is_victory(handLms):
                    victory_detected = True
                mp_draw.draw_landmarks(img_out, handLms, mp_hands.HAND_CONNECTIONS)

        # ---------------- 2. ê²Œì´ì§€ í‘œì‹œ ----------------
        draw_gauge(img_out, ratio_A_percent, 0, TARGET_A_MIN, TARGET_A_MAX, "E-M Ratio")
        draw_gauge(img_out, ratio_B_percent, 70, TARGET_B_MIN, TARGET_B_MAX, "N-M Ratio")

        total_ratio_ok = ratio_ok_A and ratio_ok_B
        all_conditions_met = face_detected and victory_detected and total_ratio_ok

        # ---------------- 3. ì¹´ìš´íŠ¸ë‹¤ìš´ ë° ìº¡ì²˜ ë¡œì§ ----------------
        if all_conditions_met:
            if not self.countdown_active:
                self.countdown_active = True
                self.countdown_start_time = time.time()
                # ìº¡ì²˜ ë©”ì‹œì§€ë¥¼ Session Stateì— ì§ì ‘ ì €ì¥í•˜ë©´ Multi-threading í™˜ê²½ì—ì„œ ë¬¸ì œê°€ ë  ìˆ˜ ìˆì–´,
                # ì´ ë¶€ë¶„ì€ ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œë§Œ ì²˜ë¦¬í•˜ê±°ë‚˜, í™”ë©´ì— í‘œì‹œí•  ìµœì¢… í”„ë ˆì„ì—ë§Œ ë„£ë„ë¡ í•©ë‹ˆë‹¤.
                # ì—¬ê¸°ì„œëŠ” UIì— ì˜í–¥ì„ ì£¼ëŠ” ë©”ì‹œì§€ ì—…ë°ì´íŠ¸ëŠ” ë©”ì¸ ìŠ¤ë ˆë“œì— ë§¡ê¸°ê¸° ìœ„í•´ ìƒëµí•©ë‹ˆë‹¤.

            elapsed = time.time() - self.countdown_start_time
            countdown_value = COUNTDOWN_TIME - elapsed

            if countdown_value <= 0:
                self.countdown_active = False
                self.captured = True
                self.last_capture_time = time.time()

                # âœ… ìº¡ì²˜ëœ ì›ë³¸ RGB ì´ë¯¸ì§€ë¥¼ result_queueì— ì „ì†¡
                self.result_queue.put(rgb)

                # âœ… ì…”í„° ì†Œë¦¬ ì‹ í˜¸ë¥¼ shutter_queueì— ì „ì†¡
                try:
                    self.shutter_queue.put(True, block=False)
                except queue.Full:
                    pass

        else:
            if self.countdown_active:
                self.countdown_active = False
                # ìƒíƒœ ë©”ì‹œì§€ ì—…ë°ì´íŠ¸ëŠ” ë©”ì¸ ìŠ¤ë ˆë“œì— ë§¡ê¹€

        # ---------------- 4. í™”ë©´ í‘œì‹œ ê°±ì‹  ----------------
        if self.countdown_active:
            # ì¹´ìš´íŠ¸ë‹¤ìš´ ìˆ«ìëŠ” ë¹„ë””ì˜¤ í”„ë ˆì„ì—ë§Œ í‘œì‹œ
            countdown_display = max(1, int(COUNTDOWN_TIME - (time.time() - self.countdown_start_time)) + 1)
            cv2.putText(img_out, f"Capturing in: {countdown_display}", (img_w // 2 - 150, img_h // 2),
                        cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 255), 4)

        status_text = (
            f"Face: {face_detected} | V: {victory_detected} | "
            f"Ratio A: {ratio_ok_A} | Ratio B: {ratio_ok_B}"
        )
        cv2.putText(img_out, status_text,
                    (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)

        return av.VideoFrame.from_ndarray(img_out, format="bgr24")


# ---------------- Streamlit ë©”ì¸ í•¨ìˆ˜ ----------------
def main():
    st.set_page_config(page_title="ë¹„ìœ¨ & V ì‚¬ì¸ ê²€ì¶œê¸°", layout="wide")

    st.title("ğŸ“¸ ë¹„ìœ¨ ìµœì í™” V-ì‚¬ì¸ ìë™ ìº¡ì²˜ ì›¹ ì•±")
    st.markdown("""
    ëª¨ë“  ì¡°ê±´(ì–¼êµ´, V-ì‚¬ì¸, ë‘ ê°€ì§€ ë¹„ìœ¨)ì´ ì¶©ì¡±ë˜ë©´ **3ì´ˆ ì¹´ìš´íŠ¸ë‹¤ìš´** í›„ ìë™ìœ¼ë¡œ ìº¡ì²˜ë©ë‹ˆë‹¤.
    ---
    """)

    SHUTTER_HTML = load_shutter_html()

    # Session State ì´ˆê¸°í™”
    if 'capture_ready' not in st.session_state:
        st.session_state.capture_ready = False
        st.session_state.captured_image_rgb = None
        st.session_state.capture_message = "ì¹´ë©”ë¼ë¥¼ ì¼œê³  ìì„¸ë¥¼ ì¡ì•„ì£¼ì„¸ìš”."
        # âœ… ì¹´ìš´íŠ¸ë‹¤ìš´ ìƒíƒœë¥¼ UIì— í‘œì‹œí•˜ê¸° ìœ„í•œ ìƒíƒœ
        st.session_state.countdown_ui_active = False

    current_message = st.session_state.get('capture_message', "ì¹´ë©”ë¼ë¥¼ ì¼œê³  ìì„¸ë¥¼ ì¡ì•„ì£¼ì„¸ìš”.")

    col1, col2 = st.columns([2, 1])

    # ---------------- I. ì›¹ìº  ìŠ¤íŠ¸ë¦¼ (col1) ----------------
    with col1:
        st.subheader("ì‹¤ì‹œê°„ ì›¹ìº  ìŠ¤íŠ¸ë¦¼ (ë¹„ì „ ì²˜ë¦¬)")
        status_placeholder = st.empty()  # ìƒíƒœ ë©”ì‹œì§€ ì¶œë ¥ì„ ìœ„í•œ placeholder

        webrtc_ctx = webrtc_streamer(
            key="media-pipe-detector",
            video_processor_factory=VideoProcessor,
            rtc_configuration=RTC_CONFIGURATION,
            media_stream_constraints={"video": True, "audio": False},
            async_processing=True,
        )
        # ì›¹ìº  ìƒíƒœ ë©”ì‹œì§€ëŠ” ì•„ë˜ ë£¨í”„ì—ì„œ ì—…ë°ì´íŠ¸

    # ---------------- II. ìº¡ì²˜ ë° ì…”í„° ì†Œë¦¬ ê°ì§€ ë£¨í”„ (ë©”ì¸ ìŠ¤ë ˆë“œ) ----------------
    if webrtc_ctx.state.playing and webrtc_ctx.video_processor:
        processor = webrtc_ctx.video_processor

        # ë¬´í•œ ë£¨í”„ ëŒ€ì‹ , Streamlit rerun() í›„ì—ëŠ” ë£¨í”„ë¥¼ ì¢…ë£Œí•˜ë„ë¡ ì²˜ë¦¬
        while True:
            # 1. ì´ë¯¸ì§€ ìº¡ì²˜ ê²°ê³¼ í í™•ì¸
            try:
                # í íƒ€ì„ì•„ì›ƒì„ ì§§ê²Œ ì„¤ì •í•˜ì—¬ UI ì—…ë°ì´íŠ¸ë¥¼ ë°©í•´í•˜ì§€ ì•Šë„ë¡ í•¨
                result_img_rgb = processor.result_queue.get(timeout=0.01)
            except queue.Empty:
                result_img_rgb = None
            except Exception:
                break

            # 2. ì…”í„° ì†Œë¦¬ ì‹ í˜¸ í í™•ì¸ ë° HTML ì‚½ì…
            try:
                # ì…”í„° ì†Œë¦¬ ì‹ í˜¸ëŠ” ì´ë¯¸ì§€ íì™€ ë…ë¦½ì ìœ¼ë¡œ ê°ì§€
                shutter_signal = processor.shutter_queue.get(timeout=0.01)
            except queue.Empty:
                shutter_signal = None
            except Exception:
                break

            # 3. ìº¡ì²˜ ì„±ê³µ ì‹œ ìƒíƒœ ì—…ë°ì´íŠ¸ ë° UI ì¬ì‹¤í–‰ (ê°€ì¥ ì¤‘ìš”)
            if result_img_rgb is not None:
                st.session_state.captured_image_rgb = result_img_rgb
                st.session_state.capture_ready = True
                st.session_state.capture_message = "âœ… ì´¬ì˜ ì„±ê³µ! ì•„ë˜ì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”."
                st.session_state.countdown_ui_active = False  # ì¹´ìš´íŠ¸ë‹¤ìš´ ë¹„í™œì„±í™”

                # ë¹„ë””ì˜¤ í”„ë¡œì„¸ì„œì˜ ìƒíƒœë¥¼ ì¬ì„¤ì • (ë‹¤ìŒ ìº¡ì²˜ ì¤€ë¹„)
                processor.captured = False

                # ì…”í„° ì†Œë¦¬ ì‹ í˜¸ê°€ ìˆìœ¼ë©´ ì¬ìƒ (íì—ì„œ êº¼ë‚¸ í›„)
                if shutter_signal:
                    components.html(SHUTTER_HTML, height=0)

                # Streamlitì„ ì¬ì‹¤í–‰í•˜ì—¬ UI ì—…ë°ì´íŠ¸
                st.rerun()
                break  # ë£¨í”„ ì¢…ë£Œ

            # 4. ì¹´ìš´íŠ¸ë‹¤ìš´ ìƒíƒœ ê°ì§€ (ë‹¨ìˆœ UI ë©”ì‹œì§€ ì—…ë°ì´íŠ¸ìš©)
            # ì¹´ìš´íŠ¸ë‹¤ìš´ì´ í™œì„±í™”ë˜ì–´ ìˆê³ , ìº¡ì²˜ëœ ìƒíƒœê°€ ì•„ë‹ˆë©°, ì´ë¯¸ì§€ íì— ì‹ í˜¸ê°€ ì—†ëŠ” ê²½ìš°
            if processor.countdown_active and not st.session_state.capture_ready:
                if not st.session_state.countdown_ui_active:
                    st.session_state.countdown_ui_active = True
                    st.session_state.capture_message = f"ì¹´ìš´íŠ¸ë‹¤ìš´ ì‹œì‘! {COUNTDOWN_TIME}ì´ˆ ìœ ì§€í•˜ì„¸ìš”."
                    # ì¹´ìš´íŠ¸ë‹¤ìš´ ì‹œì‘ ë©”ì‹œì§€ë¥¼ ë°”ë¡œ ë„ìš°ê¸° ìœ„í•´ í•œ ë²ˆë§Œ rerun (ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•´ ì§€ì–‘ë˜ê¸°ë„ í•˜ë‚˜, ì¦‰ê°ì ì¸ í”¼ë“œë°±ì„ ìœ„í•´ ì‚¬ìš©)
                    # st.rerun() # ì£¼ì„ ì²˜ë¦¬: ë§¤ë²ˆ reruní•˜ì§€ ì•Šê³ , ì•„ë˜ status_placeholderë¥¼ ì‚¬ìš©

            elif not processor.countdown_active and st.session_state.countdown_ui_active:
                # ì¹´ìš´íŠ¸ë‹¤ìš´ì´ ì¤‘ë‹¨ë¨
                st.session_state.countdown_ui_active = False
                st.session_state.capture_message = "â³ ì¡°ê±´ ë¯¸ë‹¬ë¡œ ì¹´ìš´íŠ¸ë‹¤ìš´ ì¤‘ë‹¨."
                # st.rerun() # ì£¼ì„ ì²˜ë¦¬: ë§¤ë²ˆ reruní•˜ì§€ ì•Šê³ , ì•„ë˜ status_placeholderë¥¼ ì‚¬ìš©

            # íë¥¼ í™•ì¸í•˜ëŠ” ë™ì•ˆ CPU ë¶€í•˜ë¥¼ ì¤„ì„
            time.sleep(0.01)

            # ì‹¤ì‹œê°„ ìƒíƒœ ë©”ì‹œì§€ ì—…ë°ì´íŠ¸ (rerun ì—†ì´)
            if webrtc_ctx.video_processor:
                vp = webrtc_ctx.video_processor

                if vp.countdown_active:
                    countdown_time = max(1, int(COUNTDOWN_TIME - (time.time() - vp.countdown_start_time)) + 1)
                    status_placeholder.warning(f"í˜„ì¬ ìƒíƒœ: **ì¹´ìš´íŠ¸ë‹¤ìš´ ì¤‘! {countdown_time}ì´ˆ** ë‚¨ì•˜ìŠµë‹ˆë‹¤.")
                elif st.session_state.capture_ready:
                    status_placeholder.success(f"í˜„ì¬ ìƒíƒœ: **{st.session_state.capture_message}**")
                else:
                    status_text = "ì¡°ê±´ì„ ì¶©ì¡±ì‹œì¼œì£¼ì„¸ìš”. (ì–¼êµ´, V-ì‚¬ì¸, ë¹„ìœ¨)"
                    if not vp.face_detector.process(cv2.cvtColor(webrtc_ctx.video_processor.to_ndarray(format="bgr24"),
                                                                 cv2.COLOR_BGR2RGB)).detections:
                        status_text = "ì–¼êµ´ì„ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

                    status_placeholder.info(f"í˜„ì¬ ìƒíƒœ: **{status_text}**")

    # ---------------- III. ê²°ê³¼ í‘œì‹œ (col2) ----------------
    with col2:
        st.subheader("âœ… ìº¡ì²˜ ì¡°ê±´ ë° ê²°ê³¼")
        st.markdown(
            f"""
            **âœ… ìº¡ì²˜ ì¡°ê±´ (ëª¨ë‘ ì¶©ì¡±í•´ì•¼ í•¨):**
            * **ì–¼êµ´ ê°ì§€**
            * **V-ì‚¬ì¸ ê°ì§€**
            * **ëˆˆ-ì… ë¹„ìœ¨ (A):** ${TARGET_A_MIN}\\% \sim {TARGET_A_MAX}\\%$
            * **ì½”-ì… ë¹„ìœ¨ (B):** ${TARGET_B_MIN}\\% \sim {TARGET_B_MAX}\\%$
            """
        )
        st.markdown("---")

        if st.session_state.get('capture_ready') and st.session_state.get('captured_image_rgb') is not None:
            st.success("ğŸ‰ **ìº¡ì²˜ ì™„ë£Œ!**")

            captured_img = st.session_state.captured_image_rgb

            st.image(captured_img, caption="ìµœê·¼ ìº¡ì²˜ ì´ë¯¸ì§€", use_container_width=True)

            img_bgr = cv2.cvtColor(captured_img, cv2.COLOR_RGB2BGR)
            ret, buffer = cv2.imencode(".png", img_bgr)

            if ret:
                st.download_button(
                    label="ğŸ–¼ï¸ ìº¡ì²˜ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ",
                    data=buffer.tobytes(),
                    file_name=f"capture_optimal_{int(time.time())}.png",
                    mime="image/png"
                )

            if st.button("ğŸ”„ ë‹¤ìŒ ìº¡ì²˜ ì¤€ë¹„"):
                st.session_state.capture_ready = False
                st.session_state.captured_image_rgb = None
                st.session_state.capture_message = "ì¹´ë©”ë¼ë¥¼ ì¼œê³  ìì„¸ë¥¼ ì¡ì•„ì£¼ì„¸ìš”."
                st.rerun()

        else:
            st.warning("ì•„ì§ ìº¡ì²˜ëœ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. ì¡°ê±´ì„ ì¶©ì¡±ì‹œì¼œë³´ì„¸ìš”!")


if __name__ == "__main__":
    main()