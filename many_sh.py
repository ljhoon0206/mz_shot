import time
import cv2
import numpy as np
import streamlit as st
import mediapipe as mp
import av
import queue
import base64
import streamlit.components.v1 as components

from streamlit_webrtc import (
    webrtc_streamer,
    VideoProcessorBase,
    RTCConfiguration,
    WebRtcMode,
)

mp_face = mp.solutions.face_detection


# ---------------- ì…”í„° ì†Œë¦¬ìš© HTML ìƒì„± í•¨ìˆ˜ ----------------
def load_shutter_html():
    """
    shutter.wav íŒŒì¼ì„ base64ë¡œ ì½ì–´ì„œ <audio> ìë™ ì¬ìƒ HTMLì„ ë§Œë“¤ì–´ ì¤Œ.
    shutter.wavëŠ” ì´ íŒŒì´ì¬ íŒŒì¼ê³¼ ê°™ì€ í´ë”ì— ìˆì–´ì•¼ í•¨.
    """
    try:
        with open("shutter.wav", "rb") as f:
            audio_bytes = f.read()
        audio_b64 = base64.b64encode(audio_bytes).decode("utf-8")

        html = f"""
        <audio autoplay>
            <source src="data:audio/wav;base64,{audio_b64}" type="audio/wav" />
            ë¸Œë¼ìš°ì €ê°€ audio íƒœê·¸ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        </audio>
        """
        return html
    except FileNotFoundError:
        st.error("shutter.wav íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì…”í„° ì†Œë¦¬ ì¬ìƒì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")
        return ""


def get_face_roll_angle(img_bgr):
    """BGR ì´ë¯¸ì§€ì—ì„œ ì²« ë²ˆì§¸ ì–¼êµ´ì˜ roll angle(ê¸°ìš¸ê¸°) ê³„ì‚°."""
    h, w, _ = img_bgr.shape

    with mp_face.FaceDetection(
            model_selection=0,
            min_detection_confidence=0.6
    ) as face_detector:
        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
        results = face_detector.process(img_rgb)

        if not results.detections:
            return None, None

        detection = results.detections[0]
        keypoints = detection.location_data.relative_keypoints

        right_eye = keypoints[0]
        left_eye = keypoints[1]

        x1, y1 = right_eye.x * w, right_eye.y * h
        x2, y2 = left_eye.x * w, left_eye.y * h

        dx = x2 - x1
        dy = y2 - y1

        angle_rad = np.arctan2(dy, dx)
        angle_deg = np.degrees(angle_rad)

        right_eye_pt = (int(x1), int(y1))
        left_eye_pt = (int(x2), int(y2))

        return angle_deg, (right_eye_pt, left_eye_pt)


def draw_angle_overlay(img_bgr, angle_deg, eye_pts, label=""):
    """íƒ€ê²Ÿ ì´ë¯¸ì§€ ìœ„ì— ëˆˆ ì„  + ê°ë„ í…ìŠ¤íŠ¸ í‘œì‹œ."""
    img = img_bgr.copy()
    if angle_deg is not None and eye_pts is not None:
        (re, le) = eye_pts
        cv2.line(img, re, le, (0, 255, 0), 2)
        text = f"{label} roll: {angle_deg:.1f} deg"
    else:
        text = f"{label} No face"

    cv2.putText(
        img,
        text,
        (30, 40),
        cv2.FONT_HERSHEY_SIMPLEX,
        1.0,
        (0, 0, 255),
        2,
        cv2.LINE_AA,
    )
    return img


class PoseMatchProcessor(VideoProcessorBase):
    """
    ì›¹ìº  í”„ë ˆì„ì„ ë°›ì•„ì„œ:
      - ì–¼êµ´ roll angle ê³„ì‚° ë° ë¹„êµ
      - ì¡°ê±´ ë§Œì¡± ì‹œ ìë™ ìº¡ì²˜
      - ìº¡ì²˜ ìˆœê°„ ì…”í„° ì†Œë¦¬ ì‹ í˜¸(shutter_queue) ë³´ëƒ„
    """

    def __init__(self):
        self._frame_format = "bgr24"

        self.ref_angle = None
        self.tolerance = 5.0
        self.cooldown_sec = 3.0
        self.last_capture_time = 0.0

        self.person_infos = []
        self.captured_images = []

        self.face_detector = mp_face.FaceDetection(
            model_selection=0,
            min_detection_confidence=0.6
        )

        # â­ ì…”í„° ì†Œë¦¬ íŠ¸ë¦¬ê±°ìš© í (ì¶”ê°€ë¨)
        self.shutter_queue = queue.Queue()

    def recv(self, frame):
        img = frame.to_ndarray(format="bgr24")
        raw_img = img.copy()

        h, w, _ = img.shape

        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        results = self.face_detector.process(img_rgb)

        self.person_infos = []
        faces_for_capture = []

        if results and results.detections:
            # ... (ìœ ì‚¬ë„ ê³„ì‚° ë° ê·¸ë¦¬ê¸° ë¡œì§ì€ ì´ì „ ì½”ë“œì™€ ë™ì¼í•˜ê²Œ ì§„í–‰) ...
            temp_list = []
            for det in results.detections:
                keypoints = det.location_data.relative_keypoints
                right_eye = keypoints[0]
                left_eye = keypoints[1]

                x1, y1 = right_eye.x * w, right_eye.y * h
                x2, y2 = left_eye.x * w, left_eye.y * h
                dx, dy = x2 - x1, y2 - y1

                angle_rad = np.arctan2(dy, dx)
                angle_deg = np.degrees(angle_rad)

                sim = None
                if self.ref_angle is not None:
                    diff = abs(angle_deg - self.ref_angle)
                    if diff >= self.tolerance:
                        sim = 0.0
                    else:
                        sim = max(0.0, 100.0 * (1.0 - diff / self.tolerance))

                center_x = (x1 + x2) / 2.0
                temp_list.append(
                    {
                        "det": det,
                        "angle": angle_deg,
                        "sim": sim,
                        "center_x": center_x,
                    }
                )

            temp_list.sort(key=lambda d: d["center_x"])

            for idx, info in enumerate(temp_list, start=1):
                det = info["det"]
                angle_deg = info["angle"]
                sim = info["sim"]

                self.person_infos.append(
                    {
                        "id": idx,
                        "angle": angle_deg,
                        "sim": sim,
                    }
                )

                # ì‹œê°ì  í”¼ë“œë°± (ëˆˆ ì„ , ë°•ìŠ¤, í…ìŠ¤íŠ¸, ìœ ì‚¬ë„ ë°”) ê·¸ë¦¬ê¸° ë¡œì§ (ìƒëµ)

                if sim is not None:
                    faces_for_capture.append(sim)

            # 4) ì—¬ëŸ¬ ëª… ì¤‘ í•˜ë‚˜ë¼ë„ ìœ ì‚¬ë„ ê¸°ì¤€ ë„˜ìœ¼ë©´ ìë™ ìº¡ì²˜
            if self.ref_angle is not None and faces_for_capture:
                max_sim = max(faces_for_capture)
                now = time.time()
                if max_sim >= 90.0 and now - self.last_capture_time > self.cooldown_sec:
                    self.last_capture_time = now

                    # âœ… UI ì—†ëŠ” ì›ë³¸(raw_img)ì„ ì €ì¥
                    self.captured_images.append(raw_img.copy())
                    if len(self.captured_images) > 10:
                        self.captured_images.pop(0)

                    # â­ ì…”í„° ì†Œë¦¬ ì‹ í˜¸ ë³´ë‚´ê¸° (í ì‚¬ìš©)
                    try:
                        self.shutter_queue.put(True, block=False)
                    except queue.Full:
                        pass

                    # í™”ë©´ì—ë§Œ CAPTURED! í…ìŠ¤íŠ¸ í‘œì‹œ
                    cv2.putText(
                        img,
                        "CAPTURED!",
                        (30, h - 40),
                        cv2.FONT_HERSHEY_SIMPLEX,
                        1.2,
                        (0, 0, 255),
                        3,
                        cv2.LINE_AA,
                    )

            top_text = f"Faces: {len(temp_list)}"
            if self.ref_angle is not None:
                top_text += f" | target:{self.ref_angle:.1f} deg"
        else:
            top_text = "No face"
            self.person_infos = []

        # ìƒë‹¨ ê³µí†µ í…ìŠ¤íŠ¸
        cv2.putText(
            img,
            top_text,
            (30, 40),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.8,
            (0, 255, 255),
            2,
            cv2.LINE_AA,
        )

        return av.VideoFrame.from_ndarray(img, format="bgr24")


def main():
    st.title("íƒ€ê²Ÿ í¬ì¦ˆ ìœ ì‚¬ë„ ê¸°ë°˜ ìë™ ì´¬ì˜ (ì—¬ëŸ¬ ëª… + ì „ë©´/í›„ë©´ ì§€ì›)")

    st.markdown(
        """
        **ë¬¸ì œ í•´ê²°:** ì…”í„° ì†Œë¦¬ ì§€ì—° ë¬¸ì œ í•´ê²°ì„ ìœ„í•´, ìº¡ì²˜ ì‹œ ì‹ í˜¸ë¥¼ **í**ì— ë„£ì–´ ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ **ì§€ì†ì ìœ¼ë¡œ ê°ì§€**í•˜ë„ë¡ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.
        ---
        """
    )

    # ğŸ”” ì…”í„° ì†Œë¦¬ HTML ë¯¸ë¦¬ ì¤€ë¹„
    SHUTTER_HTML = load_shutter_html()

    # ìº¡ì²˜ëœ ì´ë¯¸ì§€ ëª©ë¡ì€ Session Stateì— ì €ì¥í•˜ì—¬ UIì— ë°˜ì˜
    if "captured_images_main" not in st.session_state:
        st.session_state["captured_images_main"] = []

    # --- (ìƒëµ: íƒ€ê²Ÿ ì‚¬ì§„ ì—…ë¡œë“œ, ì´¬ì˜ ì¡°ê±´ ì„¤ì • ì½”ë“œëŠ” ì´ì „ê³¼ ë™ì¼) ---
    st.sidebar.header("â‘  íƒ€ê²Ÿ ì‚¬ì§„ ì—…ë¡œë“œ")
    ref_file = st.sidebar.file_uploader("íƒ€ê²Ÿ í¬ì¦ˆ ì‚¬ì§„ (jpg, png)", type=["jpg", "jpeg", "png"], key="ref_upload")

    ref_angle = None
    ref_disp = None

    if ref_file is not None:
        data = ref_file.read()
        arr = np.frombuffer(data, np.uint8)
        ref_img = cv2.imdecode(arr, cv2.IMREAD_COLOR)

        if ref_img is None:
            st.sidebar.error("íƒ€ê²Ÿ ì‚¬ì§„ì„ ì½ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        else:
            ref_angle, eye_pts = get_face_roll_angle(ref_img)
            if ref_angle is None:
                st.sidebar.error("íƒ€ê²Ÿ ì‚¬ì§„ì—ì„œ ì–¼êµ´ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            else:
                st.sidebar.success(f"íƒ€ê²Ÿ ì–¼êµ´ ê°ë„: {ref_angle:.1f}Â°")
                ref_disp = draw_angle_overlay(ref_img, ref_angle, eye_pts, label="target")

    st.sidebar.header("â‘¡ ì´¬ì˜ ì¡°ê±´")
    tolerance = st.sidebar.slider("í—ˆìš© ê°ë„ ì°¨ (deg)", min_value=2.0, max_value=30.0, value=8.0, step=1.0)
    cooldown_sec = st.sidebar.slider("ì´¬ì˜ ê°„ ìµœì†Œ ê°„ê²© (ì´ˆ)", min_value=0.0, max_value=10.0, value=3.0, step=1.0)

    if ref_disp is not None:
        st.subheader("íƒ€ê²Ÿ ì‚¬ì§„ (ê°ë„ í‘œì‹œ)")
        st.image(ref_disp, channels="BGR")

    rtc_config = RTCConfiguration(
        {"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
    )

    st.subheader("ì›¹ìº ")

    cam_mode = st.radio("ì¹´ë©”ë¼ ì„ íƒ", ["ì „ë©´", "í›„ë©´"], horizontal=True)

    base_constraints = {
        "width": {"ideal": 480},
        "height": {"ideal": 360},
        "frameRate": {"ideal": 15},
    }

    video_constraints = {
        **base_constraints,
        "facingMode": {"ideal": "user"} if cam_mode == "ì „ë©´" else {"ideal": "environment"},
    }

    webrtc_ctx = webrtc_streamer(
        key="pose-match-capture-multi",
        mode=WebRtcMode.SENDRECV,
        rtc_configuration=rtc_config,
        media_stream_constraints={"video": video_constraints, "audio": False},
        video_processor_factory=PoseMatchProcessor,
        async_processing=True,
    )

    # â­ ì…”í„° ì†Œë¦¬ ê°ì§€ ë° ëª©ë¡ ì—…ë°ì´íŠ¸ ë£¨í”„ (ë©”ì¸ ìŠ¤ë ˆë“œ)
    if webrtc_ctx.video_processor:
        vp: PoseMatchProcessor = webrtc_ctx.video_processor

        # íƒ€ê²Ÿ ê°ë„/ì„¤ì • ì „ë‹¬
        vp.ref_angle = ref_angle
        vp.tolerance = tolerance
        vp.cooldown_sec = cooldown_sec

        # ìº¡ì²˜ ì‹ í˜¸ ê°ì§€ ë£¨í”„
        while webrtc_ctx.state.playing:
            try:
                # â­ ì…”í„° ì†Œë¦¬ ì‹ í˜¸ í™•ì¸ (ë…¼ë¸”ë¡œí‚¹)
                if vp.shutter_queue.get(timeout=0.1):
                    components.html(SHUTTER_HTML, height=0)  # ì†Œë¦¬ ì¬ìƒ

                    # ìº¡ì²˜ëœ ì´ë¯¸ì§€ ëª©ë¡ì„ ë©”ì¸ ìŠ¤ë ˆë“œ ìƒíƒœë¡œ ë³µì‚¬
                    # ìº¡ì²˜ëœ ì´ë¯¸ì§€ì˜ ì‚¬ë³¸ì„ ì €ì¥í•˜ì—¬, ì´í›„ UI ì—…ë°ì´íŠ¸ ì‹œ ë°˜ì˜
                    st.session_state["captured_images_main"] = vp.captured_images[:]

                    # â­ ìƒíƒœê°€ ë³€ê²½ë˜ì—ˆìœ¼ë¯€ë¡œ Streamlitì„ ì¬ì‹¤í–‰í•˜ì—¬ UIë¥¼ ì—…ë°ì´íŠ¸
                    # (ì´ë•Œ ì†Œë¦¬ê°€ ë‚˜ì§€ ì•Šë„ë¡ ì´ë¯¸ íì—ì„œ ì‹ í˜¸ë¥¼ êº¼ëƒˆìŒ)
                    st.rerun()
                    break

            except queue.Empty:
                pass  # ì‹ í˜¸ ì—†ìœ¼ë©´ ê³„ì† ëŒ€ê¸°
            except Exception:
                break

            time.sleep(0.01)  # CPU ë¶€í•˜ ì¤„ì´ê¸°

        st.subheader("í˜„ì¬ ìƒíƒœ (ì™¼ìª½ë¶€í„° P1, P2, ...)")

        if ref_angle is None:
            st.warning("íƒ€ê²Ÿ ì‚¬ì§„ì„ ë¨¼ì € ì—…ë¡œë“œí•´ì•¼ ìœ ì‚¬ë„ ê³„ì‚°ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        else:
            if not vp.person_infos:
                st.write("í˜„ì¬ ì–¼êµ´ì„ ì°¾ëŠ” ì¤‘ì…ë‹ˆë‹¤.")
            else:
                for info in vp.person_infos:
                    pid = info["id"]
                    angle = info["angle"]
                    sim = info["sim"]
                    if sim is None:
                        st.write(f"ì‚¬ëŒ P{pid}: ê°ë„ **{angle:.1f}Â°** (íƒ€ê²Ÿ ì—†ìŒ)")
                    else:
                        st.write(
                            f"ì‚¬ëŒ P{pid}: ê°ë„ **{angle:.1f}Â°**, "
                            f"ìœ ì‚¬ë„(ê°ë„ ê¸°ì¤€): **{sim:.0f}%**"
                        )

        st.subheader("ìë™ ì´¬ì˜ëœ ì‚¬ì§„ë“¤")

        # ìº¡ì²˜ ëª©ë¡ì€ ì…”í„° ì†Œë¦¬ ê°ì§€ ë£¨í”„ì—ì„œ ì—…ë°ì´íŠ¸ëœ session_stateë¥¼ ì‚¬ìš©
        if st.session_state["captured_images_main"]:
            st.button("ìº¡ì²˜ ëª©ë¡ ë‹¤ì‹œ ê·¸ë¦¬ê¸°", key="refresh_ui")  # UI ê°±ì‹ ìš©

            for idx, img in enumerate(reversed(st.session_state["captured_images_main"]), start=1):
                st.image(img, channels="BGR", caption=f"ìº¡ì²˜ #{idx}")

                success, buf = cv2.imencode(".jpg", img)
                if success:
                    st.download_button(
                        label=f"ì´ ì‚¬ì§„ ë‹¤ìš´ë¡œë“œ #{idx}",
                        data=buf.tobytes(),
                        file_name=f"capture_{idx}.jpg",
                        mime="image/jpeg",
                        key=f"download_{idx}",
                    )
        else:
            st.write("ì•„ì§ ìº¡ì²˜ëœ ì‚¬ì§„ì´ ì—†ìŠµë‹ˆë‹¤. íƒ€ê²Ÿ ê°ë„ì™€ ë¹„ìŠ·í•˜ê²Œ ë§ì¶°ë³´ì„¸ìš”.")


if __name__ == "__main__":
    main()