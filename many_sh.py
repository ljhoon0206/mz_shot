import time
import cv2
import numpy as np
import streamlit as st
import mediapipe as mp
import av
import queue
import base64
import streamlit.components.v1 as components

from streamlit_webrtc import (
    webrtc_streamer,
    VideoProcessorBase,
    RTCConfiguration,
    WebRtcMode,
)

# Mediapipe Face Detection ì´ˆê¸°í™”
mp_face = mp.solutions.face_detection


# ---------------- ì…”í„° ì†Œë¦¬ìš© HTML ìƒì„± í•¨ìˆ˜ ----------------
def load_shutter_html():
    """
    shutter.wav íŒŒì¼ì„ base64ë¡œ ì½ì–´ì„œ <audio> ìë™ ì¬ìƒ HTMLì„ ë§Œë“¤ì–´ ì¤Œ.
    shutter.wavëŠ” ì´ íŒŒì´ì¬ íŒŒì¼ê³¼ ê°™ì€ í´ë”ì— ìˆì–´ì•¼ í•¨.
    """
    try:
        with open("shutter.wav", "rb") as f:
            audio_bytes = f.read()
        audio_b64 = base64.b64encode(audio_bytes).decode("utf-8")

        html = f"""
        <audio autoplay>
            <source src="data:audio/wav;base64,{audio_b64}" type="audio/wav" />
            ë¸Œë¼ìš°ì €ê°€ audio íƒœê·¸ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        </audio>
        """
        return html
    except FileNotFoundError:
        # ì´ ì—ëŸ¬ ë©”ì‹œì§€ëŠ” Streamlit UIì— í‘œì‹œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        st.error("shutter.wav íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì´ ìŠ¤í¬ë¦½íŠ¸ì™€ ê°™ì€ í´ë”ì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return ""


def get_face_roll_angle(img_bgr):
    """BGR ì´ë¯¸ì§€ì—ì„œ ì²« ë²ˆì§¸ ì–¼êµ´ì˜ roll angle(ê¸°ìš¸ê¸°) ê³„ì‚°."""
    h, w, _ = img_bgr.shape

    with mp_face.FaceDetection(
            model_selection=0,
            min_detection_confidence=0.6
    ) as face_detector:
        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
        results = face_detector.process(img_rgb)

        if not results.detections:
            return None, None

        detection = results.detections[0]
        keypoints = detection.location_data.relative_keypoints

        right_eye = keypoints[0]
        left_eye = keypoints[1]

        x1, y1 = right_eye.x * w, right_eye.y * h
        x2, y2 = left_eye.x * w, left_eye.y * h

        dx = x2 - x1
        dy = y2 - y1

        angle_rad = np.arctan2(dy, dx)
        angle_deg = np.degrees(angle_rad)

        right_eye_pt = (int(x1), int(y1))
        left_eye_pt = (int(x2), int(y2))

        return angle_deg, (right_eye_pt, left_eye_pt)


def draw_angle_overlay(img_bgr, angle_deg, eye_pts, label=""):
    """íƒ€ê²Ÿ ì´ë¯¸ì§€ ìœ„ì— ëˆˆ ì„  + ê°ë„ í…ìŠ¤íŠ¸ í‘œì‹œ."""
    img = img_bgr.copy()
    if angle_deg is not None and eye_pts is not None:
        (re, le) = eye_pts
        cv2.line(img, re, le, (0, 255, 0), 2)
        text = f"{label} roll: {angle_deg:.1f} deg"
    else:
        text = f"{label} No face"

    cv2.putText(
        img,
        text,
        (30, 40),
        cv2.FONT_HERSHEY_SIMPLEX,
        1.0,
        (0, 0, 255),
        2,
        cv2.LINE_AA,
    )
    return img


class PoseMatchProcessor(VideoProcessorBase):
    """
    ì›¹ìº  í”„ë ˆì„ì„ ë°›ì•„ì„œ:
      - ì—¬ëŸ¬ ì‚¬ëŒì˜ ì–¼êµ´ roll angle ê³„ì‚°
      - íƒ€ê²Ÿ ê°ë„ì™€ ë¹„êµí•´ ì‚¬ëŒë³„ ìœ ì‚¬ë„ ê³„ì‚°
      - ì‚¬ëŒë³„ ìœ ì‚¬ë„ ë°” í‘œì‹œ
      - ì¡°ê±´ ë§Œì¡± ì‹œ ìë™ ìº¡ì²˜ (ìµœê·¼ 10ì¥ ì €ì¥, ìº¡ì²˜ ì‚¬ì§„ì—ëŠ” UI ì—†ìŒ)
      - ìº¡ì²˜ ìˆœê°„ ì…”í„° ì†Œë¦¬ ì‹ í˜¸(shutter_queue) ë³´ëƒ„
    """

    def __init__(self):
        self._frame_format = "bgr24"

        self.ref_angle = None
        self.tolerance = 5.0
        self.cooldown_sec = 3.0
        self.last_capture_time = 0.0

        self.person_infos = []
        self.captured_images = []

        self.face_detector = mp_face.FaceDetection(
            model_selection=0,
            min_detection_confidence=0.6
        )

        # ğŸ”” ì…”í„° ì†Œë¦¬ íŠ¸ë¦¬ê±°ìš© í
        self.shutter_queue = queue.Queue()
        # ğŸ”” ìº¡ì²˜ ë°œìƒ ì‹ í˜¸ìš© í (main í•¨ìˆ˜ì—ì„œ UI ìƒˆë¡œê³ ì¹¨ì„ ìœ„í•´ ì‚¬ìš©)
        self.capture_event_queue = queue.Queue()

    def recv(self, frame):
        img = frame.to_ndarray(format="bgr24")
        raw_img = img.copy()

        h, w, _ = img.shape

        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        results = self.face_detector.process(img_rgb)

        self.person_infos = []
        faces_for_capture = []

        if results and results.detections:
            # (ì¤‘ëµ: ì–¼êµ´ ì¸ì‹ ë° ìœ ì‚¬ë„ ê³„ì‚° ë¡œì§)
            temp_list = []
            for det in results.detections:
                keypoints = det.location_data.relative_keypoints
                right_eye = keypoints[0]
                left_eye = keypoints[1]

                x1, y1 = right_eye.x * w, right_eye.y * h
                x2, y2 = left_eye.x * w, left_eye.y * h
                dx, dy = x2 - x1, y2 - y1

                angle_rad = np.arctan2(dy, dx)
                angle_deg = np.degrees(angle_rad)

                sim = None
                if self.ref_angle is not None:
                    diff = abs(angle_deg - self.ref_angle)
                    if diff >= self.tolerance:
                        sim = 0.0
                    else:
                        sim = max(0.0, 100.0 * (1.0 - diff / self.tolerance))

                center_x = (x1 + x2) / 2.0
                temp_list.append(
                    {
                        "det": det,
                        "angle": angle_deg,
                        "sim": sim,
                        "center_x": center_x,
                    }
                )

            temp_list.sort(key=lambda d: d["center_x"])

            for idx, info in enumerate(temp_list, start=1):
                det = info["det"]
                angle_deg = info["angle"]
                sim = info["sim"]

                self.person_infos.append(
                    {
                        "id": idx,
                        "angle": angle_deg,
                        "sim": sim,
                    }
                )

                # ëˆˆ ì„  ë° ì–¼êµ´ ë°•ìŠ¤ ê·¸ë¦¬ê¸° (ì¤‘ëµ)
                keypoints = det.location_data.relative_keypoints
                right_eye = keypoints[0]
                left_eye = keypoints[1]
                x1, y1 = right_eye.x * w, right_eye.y * h
                x2, y2 = left_eye.x * w, left_eye.y * h
                right_eye_pt = (int(x1), int(y1))
                left_eye_pt = (int(x2), int(y2))

                cv2.line(img, right_eye_pt, left_eye_pt, (0, 255, 0), 2)

                rel_box = det.location_data.relative_bounding_box
                bx = int(rel_box.xmin * w)
                by = int(rel_box.ymin * h)
                bw = int(rel_box.width * w)
                bh = int(rel_box.height * h)
                bx = max(0, bx)
                by = max(0, by)
                bw = max(0, bw)
                bh = max(0, bh)
                cv2.rectangle(img, (bx, by), (bx + bw, by + bh), (255, 255, 0), 1)

                if sim is not None:
                    text = f"P{idx} angle:{angle_deg:.1f} deg | sim:{sim:.0f}%"
                else:
                    text = f"P{idx} angle:{angle_deg:.1f} deg"
                cv2.putText(
                    img,
                    text,
                    (bx, max(0, by - 10)),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.5,
                    (0, 255, 255),
                    1,
                    cv2.LINE_AA,
                )

                if sim is not None:
                    # ì–¼êµ´ ì•„ë˜ ê°œì¸ ìœ ì‚¬ë„ ë°” ê·¸ë¦¬ê¸° (ì¤‘ëµ)
                    bar_x1 = bx
                    bar_y1 = by + bh + 10
                    bar_x2 = bx + bw
                    bar_y2 = bar_y1 + 10

                    bar_x1 = max(0, min(bar_x1, w - 1))
                    bar_x2 = max(0, min(bar_x2, w - 1))
                    bar_y1 = max(0, min(bar_y1, h - 1))
                    bar_y2 = max(0, min(bar_y2, h - 1))

                    if bar_x2 > bar_x1 and bar_y2 > bar_y1:
                        cv2.rectangle(
                            img,
                            (bar_x1, bar_y1),
                            (bar_x2, bar_y2),
                            (80, 80, 80),
                            1,
                        )
                        ratio = max(0.0, min(1.0, sim / 100.0))
                        fill_x2 = bar_x1 + int((bar_x2 - bar_x1) * ratio)
                        cv2.rectangle(
                            img,
                            (bar_x1, bar_y1),
                            (fill_x2, bar_y2),
                            (0, 200, 0),
                            -1,
                        )

                if sim is not None:
                    faces_for_capture.append(sim)

            # 4) ì—¬ëŸ¬ ëª… ì¤‘ í•˜ë‚˜ë¼ë„ ìœ ì‚¬ë„ ê¸°ì¤€ ë„˜ìœ¼ë©´ ìë™ ìº¡ì²˜
            if self.ref_angle is not None and faces_for_capture:
                max_sim = max(faces_for_capture)
                now = time.time()
                if max_sim >= 90.0 and now - self.last_capture_time > self.cooldown_sec:
                    self.last_capture_time = now

                    # âœ… UI ì—†ëŠ” ì›ë³¸(raw_img)ì„ ì €ì¥
                    self.captured_images.append(raw_img.copy())
                    if len(self.captured_images) > 10:
                        self.captured_images.pop(0)

                    # ğŸ”” ì…”í„° ì†Œë¦¬ ì‹ í˜¸ ë³´ë‚´ê¸°
                    try:
                        self.shutter_queue.put_nowait(True)
                    except queue.Full:
                        pass

                    # ğŸ”” ìº¡ì²˜ ë°œìƒ ì‹ í˜¸ ë³´ë‚´ê¸° (main í•¨ìˆ˜ì—ì„œ ìº¡ì²˜ ëª©ë¡ ìƒˆë¡œê³ ì¹¨ì„ ìœ„í•´ ì‚¬ìš©)
                    try:
                        self.capture_event_queue.put_nowait(True)
                    except queue.Full:
                        pass

                    # í™”ë©´ì—ë§Œ CAPTURED! í…ìŠ¤íŠ¸ í‘œì‹œ
                    cv2.putText(
                        img,
                        "CAPTURED!",
                        (30, h - 40),
                        cv2.FONT_HERSHEY_SIMPLEX,
                        1.2,
                        (0, 0, 255),
                        3,
                        cv2.LINE_AA,
                    )

            top_text = f"Faces: {len(temp_list)}"
            if self.ref_angle is not None:
                top_text += f" | target:{self.ref_angle:.1f} deg"
        else:
            top_text = "No face"
            self.person_infos = []

        # ìƒë‹¨ ê³µí†µ í…ìŠ¤íŠ¸
        cv2.putText(
            img,
            top_text,
            (30, 40),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.8,
            (0, 255, 255),
            2,
            cv2.LINE_AA,
        )

        return av.VideoFrame.from_ndarray(img, format="bgr24")


def main():
    # ìº¡ì²˜ ëª©ë¡ì„ ê´€ë¦¬í•˜ê¸° ìœ„í•œ placeholder ì„¤ì •
    if 'capture_placeholder' not in st.session_state:
        st.session_state.capture_placeholder = st.empty()

    st.title("íƒ€ê²Ÿ í¬ì¦ˆ ìœ ì‚¬ë„ ê¸°ë°˜ ìë™ ì´¬ì˜ (ì—¬ëŸ¬ ëª… + ì „ë©´/í›„ë©´ ì§€ì›)")
    # (ì¤‘ëµ: ì•ˆë‚´ ë¬¸êµ¬)

    # ğŸ”” ì…”í„° ì†Œë¦¬ HTML ë¯¸ë¦¬ ì¤€ë¹„
    SHUTTER_HTML = load_shutter_html()

    # --- íƒ€ê²Ÿ ì‚¬ì§„ ì—…ë¡œë“œ --- (ì¤‘ëµ)
    st.sidebar.header("â‘  íƒ€ê²Ÿ ì‚¬ì§„ ì—…ë¡œë“œ")
    ref_file = st.sidebar.file_uploader(
        "íƒ€ê²Ÿ í¬ì¦ˆ ì‚¬ì§„ (jpg, png)",
        type=["jpg", "jpeg", "png"],
        key="ref_upload",
    )

    ref_angle = None
    ref_disp = None

    if ref_file is not None:
        data = ref_file.read()
        arr = np.frombuffer(data, np.uint8)
        ref_img = cv2.imdecode(arr, cv2.IMREAD_COLOR)

        if ref_img is None:
            st.sidebar.error("íƒ€ê²Ÿ ì‚¬ì§„ì„ ì½ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        else:
            ref_angle, eye_pts = get_face_roll_angle(ref_img)
            if ref_angle is None:
                st.sidebar.error("íƒ€ê²Ÿ ì‚¬ì§„ì—ì„œ ì–¼êµ´ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            else:
                st.sidebar.success(f"íƒ€ê²Ÿ ì–¼êµ´ ê°ë„: {ref_angle:.1f}Â°")
                ref_disp = draw_angle_overlay(ref_img, ref_angle, eye_pts, label="target")

    # --- ì´¬ì˜ ì¡°ê±´ --- (ì¤‘ëµ)
    st.sidebar.header("â‘¡ ì´¬ì˜ ì¡°ê±´")
    tolerance = st.sidebar.slider(
        "í—ˆìš© ê°ë„ ì°¨ (deg)",
        min_value=2.0,
        max_value=30.0,
        value=8.0,
        step=1.0,
    )
    cooldown_sec = st.sidebar.slider(
        "ì´¬ì˜ ê°„ ìµœì†Œ ê°„ê²© (ì´ˆ)",
        min_value=0.0,
        max_value=10.0,
        value=3.0,
        step=1.0,
    )

    if ref_disp is not None:
        st.subheader("íƒ€ê²Ÿ ì‚¬ì§„ (ê°ë„ í‘œì‹œ)")
        st.image(ref_disp, channels="BGR")

    rtc_config = RTCConfiguration(
        {"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
    )

    st.subheader("ì›¹ìº ")

    # ì „ë©´ / í›„ë©´ ì¹´ë©”ë¼ ì„ íƒ (ëª¨ë°”ì¼ì—ì„œ ìœ íš¨)
    cam_mode = st.radio(
        "ì¹´ë©”ë¼ ì„ íƒ",
        ["ì „ë©´", "í›„ë©´"],
        horizontal=True,
    )

    # í•´ìƒë„ëŠ” ì¡°ê¸ˆ ë‚®ê²Œ (480x360) â†’ ì „ì†¡/ë””ì½”ë”© ì•ˆì •ì„± â†‘
    base_constraints = {
        "width": {"ideal": 480},
        "height": {"ideal": 360},
        "frameRate": {"ideal": 15},
    }

    if cam_mode == "ì „ë©´":
        video_constraints = {
            **base_constraints,
            "facingMode": {"ideal": "user"},
        }
    else:  # í›„ë©´
        video_constraints = {
            **base_constraints,
            "facingMode": {"ideal": "environment"},
        }

    webrtc_ctx = webrtc_streamer(
        key="pose-match-capture-multi",
        mode=WebRtcMode.SENDRECV,
        rtc_configuration=rtc_config,
        media_stream_constraints={
            "video": video_constraints,
            "audio": False,
        },
        video_processor_factory=PoseMatchProcessor,
        async_processing=True,
    )

    # ---------------- ìº¡ì²˜ ëª©ë¡ì„ ë‹¤ì‹œ ê·¸ë¦¬ëŠ” í•¨ìˆ˜ ----------------
    def redraw_captured_images(vp: PoseMatchProcessor):
        with st.session_state.capture_placeholder.container():
            st.subheader("ìë™ ì´¬ì˜ëœ ì‚¬ì§„ë“¤")

            # ìº¡ì²˜ ëª©ë¡ ìƒˆë¡œê³ ì¹¨ ë²„íŠ¼ì„ ëˆ„ë¥´ë”ë¼ë„ ìº¡ì²˜ê°€ ì¬ë°œìƒí•˜ì§€ ì•Šë„ë¡ st.rerun() ì—†ì´ ë¡œì§ë§Œ ìˆ˜í–‰
            if st.button("ìº¡ì²˜ ëª©ë¡ ìƒˆë¡œê³ ì¹¨"):
                pass

            if vp.captured_images:
                for idx, img in enumerate(reversed(vp.captured_images), start=1):
                    st.image(img, channels="BGR", caption=f"ìº¡ì²˜ #{idx}")

                    success, buf = cv2.imencode(".jpg", img)
                    if success:
                        st.download_button(
                            label=f"ì´ ì‚¬ì§„ ë‹¤ìš´ë¡œë“œ #{idx}",
                            data=buf.tobytes(),
                            file_name=f"capture_{idx}.jpg",
                            mime="image/jpeg",
                            key=f"download_{idx}",
                        )
            else:
                st.write("ì•„ì§ ìº¡ì²˜ëœ ì‚¬ì§„ì´ ì—†ìŠµë‹ˆë‹¤. íƒ€ê²Ÿ ê°ë„ì™€ ë¹„ìŠ·í•˜ê²Œ ë§ì¶°ë³´ì„¸ìš”.")

    if webrtc_ctx.video_processor:
        vp: PoseMatchProcessor = webrtc_ctx.video_processor

        # íƒ€ê²Ÿ ê°ë„/ì„¤ì • ì „ë‹¬
        vp.ref_angle = ref_angle
        vp.tolerance = tolerance
        vp.cooldown_sec = cooldown_sec

        # ---------------- ì…”í„° ì†Œë¦¬ ë° ìº¡ì²˜ ì´ë²¤íŠ¸ ê°ì§€ ë¡œì§ ----------------
        shutter_sound_triggered = False
        capture_list_updated = False

        try:
            # ğŸ”” ì…”í„° í í™•ì¸ â†’ ì‹ í˜¸ ìˆìœ¼ë©´ ì†Œë¦¬ ì¬ìƒ
            if vp.shutter_queue.get_nowait():
                components.html(SHUTTER_HTML, height=0)
                shutter_sound_triggered = True
        except queue.Empty:
            pass

        try:
            # ğŸ”” ìº¡ì²˜ ì´ë²¤íŠ¸ í í™•ì¸ â†’ ì‹ í˜¸ ìˆìœ¼ë©´ ìº¡ì²˜ ëª©ë¡ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ UI ìƒˆë¡œê³ ì¹¨
            if vp.capture_event_queue.get_nowait():
                capture_list_updated = True
        except queue.Empty:
            pass

        # ìº¡ì²˜ ëª©ë¡ì„ í‘œì‹œí•©ë‹ˆë‹¤.
        # ì£¼ì˜: ì´ ë¡œì§ì€ Streamlitì˜ UI ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ ë©”ì¸ ë£¨í”„ì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤.
        redraw_captured_images(vp)

        # ìº¡ì²˜ê°€ ë°œìƒí•œ ê²½ìš°, ìº¡ì²˜ ëª©ë¡ì„ ì—…ë°ì´íŠ¸í•˜ê¸° ìœ„í•´ Streamlitì„ ì¬ì‹¤í–‰í•©ë‹ˆë‹¤.
        # ìº¡ì²˜ ë²„íŠ¼ ëˆ„ë¦„ìœ¼ë¡œ ì¸í•œ ì¬ì‹¤í–‰ì´ ì•„ë‹ˆë¯€ë¡œ ìº¡ì²˜ê°€ ì¬ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        if capture_list_updated:
            st.rerun()

        # ---------------- í˜„ì¬ ìƒíƒœ í‘œì‹œ (ê¸°ì¡´ ì½”ë“œ ìœ ì§€) ----------------
        st.subheader("í˜„ì¬ ìƒíƒœ (ì™¼ìª½ë¶€í„° P1, P2, ...)")

        if ref_angle is None:
            st.warning("íƒ€ê²Ÿ ì‚¬ì§„ì„ ë¨¼ì € ì—…ë¡œë“œí•´ì•¼ ìœ ì‚¬ë„ ê³„ì‚°ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        else:
            if not vp.person_infos:
                st.write("í˜„ì¬ ì–¼êµ´ì„ ì°¾ëŠ” ì¤‘ì…ë‹ˆë‹¤.")
            else:
                for info in vp.person_infos:
                    pid = info["id"]
                    angle = info["angle"]
                    sim = info["sim"]
                    if sim is None:
                        st.write(f"ì‚¬ëŒ P{pid}: ê°ë„ **{angle:.1f}Â°** (íƒ€ê²Ÿ ì—†ìŒ)")
                    else:
                        st.write(
                            f"ì‚¬ëŒ P{pid}: ê°ë„ **{angle:.1f}Â°**, "
                            f"ìœ ì‚¬ë„(ê°ë„ ê¸°ì¤€): **{sim:.0f}%**"
                        )


if __name__ == "__main__":
    main()