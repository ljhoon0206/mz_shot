import streamlit as st
import cv2
import mediapipe as mp
import time
import numpy as np
import av
import queue
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase, RTCConfiguration
import streamlit.components.v1 as components
import base64

# ---------------- Mediapipe ì´ˆê¸°í™” ----------------
mp_face = mp.solutions.face_detection
mp_hands = mp.solutions.hands
mp_draw = mp.solutions.drawing_utils

# ì „ì—­ ê°ì²´ë¥¼ í•¨ìˆ˜ ë°–ì—ì„œ ì´ˆê¸°í™” (ì„±ëŠ¥ ìµœì í™”)
FACE_DETECTOR = mp_face.FaceDetection(model_selection=0, min_detection_confidence=0.4)
HAND_DETECTOR = mp_hands.Hands(max_num_hands=2, min_detection_confidence=0.5)

# ---------------- ì›¹ ì•± ì „ì—­ ë³€ìˆ˜ ì„¤ì • ----------------
TARGET_A_MIN, TARGET_A_MAX = 43, 47  # ëˆˆ-ì… ë¹„ìœ¨ (%)
TARGET_B_MIN, TARGET_B_MAX = 12, 15  # ì½”-ì… ë¹„ìœ¨ (%)
COUNTDOWN_TIME = 3.0  # ì¹´ìš´íŠ¸ë‹¤ìš´ ì‹œê°„ ì„¤ì •

# STUN ì„œë²„ (ê²€ì€ í™”ë©´ ë°©ì§€)
RTC_CONFIGURATION = RTCConfiguration(
    {"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
)


# ---------------- Victory ì œìŠ¤ì²˜ íŒë‹¨ ----------------
def is_victory(lms):
    """ê²€ì§€+ì¤‘ì§€ í´ì§, ì•½ì§€+ìƒˆë¼ ì ‘í˜ì´ë©´ V ì‚¬ì¸ True"""
    try:
        # ì†ê°€ë½ ë(tip)ì˜ Y ì¢Œí‘œê°€ ë§ˆë””(knuckle)ì˜ Y ì¢Œí‘œë³´ë‹¤ ì‘ìœ¼ë©´ (ìœ„ìª½ì— ìˆìœ¼ë©´) í´ì§„ ìƒíƒœ (Mediapipe ê¸°ì¤€)
        return (
                lms.landmark[8].y < lms.landmark[5].y and  # ê²€ì§€
                lms.landmark[12].y < lms.landmark[9].y and  # ì¤‘ì§€
                lms.landmark[16].y > lms.landmark[13].y and  # ì•½ì§€ (ì ‘í˜)
                lms.landmark[20].y > lms.landmark[17].y  # ìƒˆë¼ (ì ‘í˜)
        )
    except Exception:
        return False


# ---------------- ë¹„ìœ¨ ê³„ì‚° í•¨ìˆ˜ ----------------
def get_face_distances(detection, img_h):
    """ì–¼êµ´ ì¸ì‹ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëˆˆ-ì…, ì½”-ì… ë¹„ìœ¨ì„ ê³„ì‚°."""
    keypoints = detection.location_data.relative_keypoints
    bbox_h = detection.location_data.relative_bounding_box.height

    if bbox_h == 0:
        return {'eye_mouth_ratio': 0.0, 'nose_mouth_ratio': 0.0}

    # ì£¼ìš” í‚¤í¬ì¸íŠ¸ Y ì¢Œí‘œ (ìƒëŒ€ ì¢Œí‘œ)
    y_eye_r = keypoints[1].y
    y_eye_l = keypoints[0].y
    y_eye_center = (y_eye_r + y_eye_l) / 2
    y_mouth = keypoints[3].y
    y_nose = keypoints[2].y

    # ëˆˆ ì¤‘ì•™ - ì… (Normalized Distance)
    distance_eye_mouth_norm = abs(y_mouth - y_eye_center)
    eye_mouth_ratio = distance_eye_mouth_norm / bbox_h

    # ì½” - ì… (Normalized Distance)
    distance_nose_mouth_norm = abs(y_mouth - y_nose)
    nose_mouth_ratio = distance_nose_mouth_norm / bbox_h

    return {
        'eye_mouth_ratio': eye_mouth_ratio,
        'nose_mouth_ratio': nose_mouth_ratio
    }


# ---------------- ê²Œì´ì§€ ê·¸ë¦¬ê¸° í•¨ìˆ˜ ----------------
def draw_gauge(img, ratio_percent, x_offset, target_min, target_max, label):
    """í™”ë©´ ì™¼ìª½ì— ìˆ˜ì§ ê²Œì´ì§€ë¥¼ ê·¸ë¦½ë‹ˆë‹¤."""
    gauge_x, gauge_y = 50 + x_offset, 80
    gauge_w, gauge_h = 20, 200

    # 0~100% ì‚¬ì´ë¡œ ê°’ ì œí•œ
    ratio_percent_clamped = max(0, min(100, ratio_percent))
    is_target = target_min <= ratio_percent_clamped <= target_max

    target_color = (0, 255, 0)
    base_color = (255, 255, 255)
    fill_color = target_color if is_target else (0, 0, 255)

    cv2.rectangle(img, (gauge_x, gauge_y), (gauge_x + gauge_w, gauge_y + gauge_h), base_color, 2)

    # ê²Œì´ì§€ ì±„ìš°ê¸°
    fill_height = int(gauge_h * (ratio_percent_clamped / 100))
    fill_y_start = gauge_y + gauge_h - fill_height
    cv2.rectangle(img, (gauge_x, fill_y_start), (gauge_x + gauge_w, gauge_y + gauge_h), fill_color, cv2.FILLED)

    # íƒ€ê²Ÿ ì˜ì—­ í‘œì‹œ (ë…¸ë€ìƒ‰)
    y_max = gauge_y + gauge_h - int(gauge_h * (target_min / 100))
    y_min = gauge_y + gauge_h - int(gauge_h * (target_max / 100))
    cv2.rectangle(img, (gauge_x - 5, y_min), (gauge_x + gauge_w + 5, y_max), (0, 255, 255), 1)

    cv2.putText(img, label, (gauge_x - 10, gauge_y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, base_color, 1)
    cv2.putText(img, f"{ratio_percent_clamped}%", (gauge_x - 10, gauge_y + gauge_h + 30),
                cv2.FONT_HERSHEY_SIMPLEX, 0.6, base_color, 2)

    return is_target, ratio_percent_clamped


# ---------------- ì…”í„° ì†Œë¦¬ìš© HTML ìƒì„± í•¨ìˆ˜ ----------------
def load_shutter_html():
    """
    shutter.wav íŒŒì¼ì„ base64ë¡œ ì½ì–´ì„œ <audio> ìë™ ì¬ìƒ HTMLì„ ë§Œë“¤ì–´ ì¤Œ.
    shutter.wavëŠ” ì´ íŒŒì´ì¬ íŒŒì¼ê³¼ ê°™ì€ í´ë”ì— ìˆì–´ì•¼ í•¨.
    """
    try:
        # NOTE: ì´ íŒŒì¼ì€ ì‹¤ì œ í™˜ê²½ì— ìˆì–´ì•¼ ì†Œë¦¬ê°€ ë‚©ë‹ˆë‹¤.
        with open("shutter.wav", "rb") as f:
            audio_bytes = f.read()
        audio_b64 = base64.b64encode(audio_bytes).decode("utf-8")

        html = f"""
        <audio autoplay>
            <source src="data:audio/wav;base64,{audio_b64}" type="audio/wav" />
            ë¸Œë¼ìš°ì €ê°€ audio íƒœê·¸ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        </audio>
        """
        return html
    except FileNotFoundError:
        # íŒŒì¼ì´ ì—†ì„ ê²½ìš° ê²½ê³  ëŒ€ì‹  ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
        return ""


# ---------------- VideoProcessor í´ë˜ìŠ¤ (í•µì‹¬) ----------------
class VideoProcessor(VideoTransformerBase):
    def __init__(self):
        # ìº¡ì²˜ëœ RGB ì´ë¯¸ì§€ë¥¼ ë©”ì¸ ìŠ¤ë ˆë“œë¡œ ì „ì†¡
        self.result_queue = queue.Queue(maxsize=1)
        # ì…”í„° ì†Œë¦¬ ì‹ í˜¸ë¥¼ ë©”ì¸ ìŠ¤ë ˆë“œë¡œ ì „ì†¡
        self.shutter_queue = queue.Queue(maxsize=1)

        self.captured = False
        self.last_capture_time = 0
        self.countdown_active = False
        self.countdown_start_time = 0
        self.face_detector = FACE_DETECTOR
        self.hand_detector = HAND_DETECTOR

    def recv(self, frame: av.VideoFrame) -> av.VideoFrame:
        img = frame.to_ndarray(format="bgr24")
        img_h, img_w, _ = img.shape
        rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        # ìº¡ì²˜ ì§í›„ëŠ” í™€ë“œ ìƒíƒœ
        if self.captured:
            cv2.putText(img, "CAPTURED! (Hold)", (img_w // 2 - 150, img_h // 2 + 50),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 3)
            return av.VideoFrame.from_ndarray(img, format="bgr24")

        # ---------------- 1. ì–¼êµ´/ì† ì¸ì‹ ë° ë¹„ìœ¨ í™•ì¸ ----------------
        face_detected = False
        ratio_ok_A, ratio_ok_B = False, False
        ratio_A_percent, ratio_B_percent = 0, 0
        victory_detected = False

        img_out = img.copy()

        face_res = self.face_detector.process(rgb)
        if face_res.detections:
            face_detected = True
            d = face_res.detections[0]
            current_ratios = get_face_distances(d, img_h)
            ratio_A_percent = int(current_ratios['eye_mouth_ratio'] * 100)
            ratio_B_percent = int(current_ratios['nose_mouth_ratio'] * 100)
            ratio_ok_A = TARGET_A_MIN <= ratio_A_percent <= TARGET_A_MAX
            ratio_ok_B = TARGET_B_MIN <= ratio_B_percent <= TARGET_B_MAX
            mp_draw.draw_detection(img_out, d)

        hand_res = self.hand_detector.process(rgb)
        if hand_res.multi_hand_landmarks:
            for handLms in hand_res.multi_hand_landmarks:
                if is_victory(handLms):
                    victory_detected = True
                mp_draw.draw_landmarks(img_out, handLms, mp_hands.HAND_CONNECTIONS)

        # ---------------- 2. ê²Œì´ì§€ í‘œì‹œ ----------------
        draw_gauge(img_out, ratio_A_percent, 0, TARGET_A_MIN, TARGET_A_MAX, "E-M Ratio")
        draw_gauge(img_out, ratio_B_percent, 70, TARGET_B_MIN, TARGET_B_MAX, "N-M Ratio")

        total_ratio_ok = ratio_ok_A and ratio_ok_B
        all_conditions_met = face_detected and victory_detected and total_ratio_ok

        # ---------------- 3. ì¹´ìš´íŠ¸ë‹¤ìš´ ë° ìº¡ì²˜ ë¡œì§ ----------------
        if all_conditions_met:
            if not self.countdown_active:
                self.countdown_active = True
                self.countdown_start_time = time.time()
                # Streamlit Session State ë©”ì‹œì§€ ì—…ë°ì´íŠ¸
                st.session_state.capture_message = f"ì¹´ìš´íŠ¸ë‹¤ìš´ ì‹œì‘! {COUNTDOWN_TIME}ì´ˆ ìœ ì§€í•˜ì„¸ìš”."

            elapsed = time.time() - self.countdown_start_time
            countdown_value = COUNTDOWN_TIME - elapsed

            if countdown_value <= 0:
                self.countdown_active = False
                self.captured = True
                self.last_capture_time = time.time()

                # â­ï¸ ì›ë³¸ RGB ì´ë¯¸ì§€ë¥¼ íì— ì „ì†¡ (ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ ì²˜ë¦¬)
                try:
                    self.result_queue.put(rgb.copy(), block=False)
                except queue.Full:
                    pass

                # â­ï¸ ì…”í„° ì†Œë¦¬ ì‹ í˜¸ ì „ì†¡
                try:
                    self.shutter_queue.put(True, block=False)
                except queue.Full:
                    pass

            # ì¹´ìš´íŠ¸ë‹¤ìš´ í‘œì‹œ
            countdown_display = max(1, int(COUNTDOWN_TIME - (time.time() - self.countdown_start_time)) + 1)
            cv2.putText(img_out, f"Capturing in: {countdown_display}", (img_w // 2 - 150, img_h // 2),
                        cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 255), 4)

        else:
            if self.countdown_active:
                self.countdown_active = False
                st.session_state.capture_message = "â³ ì¡°ê±´ ë¯¸ë‹¬ë¡œ ì¹´ìš´íŠ¸ë‹¤ìš´ ì¤‘ë‹¨."

            if not self.captured and not self.countdown_active:
                # ì–¼êµ´ì´ë‚˜ V ì‚¬ì¸ì´ ê°ì§€ë˜ë©´ ìì„¸ ì•ˆë‚´ ë©”ì‹œì§€ë¥¼ ì¢€ ë” ëª…í™•íˆ í•¨
                if face_detected or victory_detected:
                    st.session_state.capture_message = "ì¡°ê±´ì„ ì¶©ì¡±ì‹œì¼œì£¼ì„¸ìš”. (ë¹„ìœ¨/Vì‚¬ì¸)"
                else:
                    st.session_state.capture_message = "ì¹´ë©”ë¼ë¥¼ ì¼œê³  ìì„¸ë¥¼ ì¡ì•„ì£¼ì„¸ìš”."

        # ---------------- 4. ë””ë²„ê·¸/ìƒíƒœ í‘œì‹œ ê°±ì‹  ----------------
        status_text = (
            f"Face: {face_detected} | V: {victory_detected} | "
            f"Ratio A: {ratio_ok_A} | Ratio B: {ratio_ok_B}"
        )
        cv2.putText(img_out, status_text,
                    (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)

        return av.VideoFrame.from_ndarray(img_out, format="bgr24")


# ---------------- Streamlit ë©”ì¸ í•¨ìˆ˜ ----------------
def main():
    st.set_page_config(page_title="ë¹„ìœ¨ & V ì‚¬ì¸ ê²€ì¶œê¸°", layout="wide")

    st.title("ğŸ“¸ ë¹„ìœ¨ ìµœì í™” V-ì‚¬ì¸ ìë™ ìº¡ì²˜ ì›¹ ì•±")
    st.markdown("""
    ëª¨ë“  ì¡°ê±´ (**ì–¼êµ´ ê°ì§€, V-ì‚¬ì¸, ìµœì  ë¹„ìœ¨**)ì´ ì¶©ì¡±ë˜ë©´ **3ì´ˆ ì¹´ìš´íŠ¸ë‹¤ìš´** í›„ ìë™ìœ¼ë¡œ ìº¡ì²˜ë˜ë©°, ì…”í„° ì†Œë¦¬ê°€ ë‚©ë‹ˆë‹¤.
    ---
    """)

    SHUTTER_HTML = load_shutter_html()
    if not SHUTTER_HTML:
        st.warning("âš ï¸ **ê²½ê³ **: `shutter.wav` íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ì…”í„° ì†Œë¦¬ ì¬ìƒì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")

    # Session State ì´ˆê¸°í™”
    if 'capture_ready' not in st.session_state:
        st.session_state.capture_ready = False
        st.session_state.captured_image_rgb = None
        st.session_state.capture_message = "ì¹´ë©”ë¼ë¥¼ ì¼œê³  ìì„¸ë¥¼ ì¡ì•„ì£¼ì„¸ìš”."

    current_message = st.session_state.get('capture_message', "ì¹´ë©”ë¼ë¥¼ ì¼œê³  ìì„¸ë¥¼ ì¡ì•„ì£¼ì„¸ìš”.")

    col1, col2 = st.columns([2, 1])

    # ---------------- I. ì›¹ìº  ìŠ¤íŠ¸ë¦¼ (col1) ----------------
    with col1:
        st.subheader("ì‹¤ì‹œê°„ ì›¹ìº  ìŠ¤íŠ¸ë¦¼ (ë¹„ì „ ì²˜ë¦¬)")

        webrtc_ctx = webrtc_streamer(
            key="media-pipe-detector",
            video_processor_factory=VideoProcessor,
            rtc_configuration=RTC_CONFIGURATION,
            media_stream_constraints={"video": True, "audio": False},
            async_processing=True,
        )

        if webrtc_ctx.state.playing:
            if "ì´¬ì˜ ì„±ê³µ" in current_message or st.session_state.get('capture_ready'):
                st.success(f"í˜„ì¬ ìƒíƒœ: **{current_message}**")
            elif "ì¹´ìš´íŠ¸ë‹¤ìš´ ì‹œì‘" in current_message:
                st.warning(f"í˜„ì¬ ìƒíƒœ: **{current_message}**")
            else:
                st.info(f"í˜„ì¬ ìƒíƒœ: **{current_message}**")
        else:
            st.warning("ì¹´ë©”ë¼ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤...")

    # â­ II. ìº¡ì²˜ ë° ì…”í„° ì†Œë¦¬ ê°ì§€ ë£¨í”„ (ë©”ì¸ ìŠ¤ë ˆë“œ)
    if webrtc_ctx.state.playing and webrtc_ctx.video_processor:
        processor = webrtc_ctx.video_processor

        # ìº¡ì²˜ ë° ì‹ í˜¸ ê°ì§€ ë£¨í”„
        while webrtc_ctx.state.playing:

            # 1. ìº¡ì²˜ ì´ë¯¸ì§€ ì‹ í˜¸ í™•ì¸
            result_img_rgb = None
            try:
                # íì—ì„œ ìº¡ì²˜ëœ RGB ì´ë¯¸ì§€ë¥¼ ê°€ì ¸ì˜´
                result_img_rgb = processor.result_queue.get(timeout=0.01)
            except queue.Empty:
                pass
            except Exception:
                break

            # 2. ì…”í„° ì†Œë¦¬ ì‹ í˜¸ í™•ì¸ ë° HTML ì‚½ì…
            try:
                if processor.shutter_queue.get(timeout=0.01):
                    if SHUTTER_HTML:
                        components.html(SHUTTER_HTML, height=0)  # base64 audio ìë™ ì¬ìƒ
            except queue.Empty:
                pass
            except Exception:
                pass

            # 3. ìº¡ì²˜ ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ Session State ì—…ë°ì´íŠ¸ í›„ UI ê°±ì‹ 
            if result_img_rgb is not None:
                st.session_state.captured_image_rgb = result_img_rgb
                st.session_state.capture_ready = True
                st.session_state.capture_message = "âœ… ì´¬ì˜ ì„±ê³µ! ì•„ë˜ì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”."

                # í”„ë¡œì„¸ì„œì˜ í™€ë“œ ìƒíƒœ í•´ì œ (ë‹¤ìŒ í”„ë ˆì„ë¶€í„° ì •ìƒ ì‘ë™)
                processor.captured = False

                # UI ì¦‰ì‹œ ê°±ì‹ 
                st.rerun()
                # st.rerun()ì´ í˜¸ì¶œë˜ë©´ ì´ ë£¨í”„ëŠ” ì¢…ë£Œë¨

            time.sleep(0.01)  # CPU ë¶€í•˜ ì¤„ì´ê¸°

    # ---------------- III. ê²°ê³¼ í‘œì‹œ (col2) ----------------
    with col2:
        st.subheader("âœ… ìº¡ì²˜ ì¡°ê±´ ë° ê²°ê³¼")
        st.markdown(
            f"""
            **âœ… ìµœì  ë¹„ìœ¨ ëª©í‘œ:**
            * **ëˆˆ-ì… ë¹„ìœ¨ (A):** ${TARGET_A_MIN}\\% \sim {TARGET_A_MAX}\\%$
            * **ì½”-ì… ë¹„ìœ¨ (B):** ${TARGET_B_MIN}\\% \sim {TARGET_B_MAX}\\%$
            * **ì¶”ê°€ ì¡°ê±´:** **ì–¼êµ´ ê°ì§€** ë° **V-ì‚¬ì¸ ê°ì§€**
            """
        )
        st.markdown("---")

        if st.session_state.get('capture_ready') and st.session_state.get('captured_image_rgb') is not None:
            st.success("ğŸ‰ **ìº¡ì²˜ ì™„ë£Œ!**")

            captured_img_rgb = st.session_state.captured_image_rgb

            # ìº¡ì²˜ ì´ë¯¸ì§€ í‘œì‹œ (RGB)
            st.image(captured_img_rgb, caption="ìµœê·¼ ìº¡ì²˜ ì´ë¯¸ì§€", use_container_width=True)

            # ë‹¤ìš´ë¡œë“œë¥¼ ìœ„í•´ RGBë¥¼ BGRë¡œ ë³€í™˜ í›„ PNG ì¸ì½”ë”©
            img_bgr = cv2.cvtColor(captured_img_rgb, cv2.COLOR_RGB2BGR)
            ret, buffer = cv2.imencode(".png", img_bgr)

            if ret:
                st.download_button(
                    label="ğŸ–¼ï¸ ìº¡ì²˜ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ",
                    data=buffer.tobytes(),
                    file_name=f"capture_optimal_{time.strftime('%Y%m%d_%H%M%S')}.png",
                    mime="image/png"
                )

            st.markdown("---")

            if st.button("ğŸ”„ ë‹¤ìŒ ìº¡ì²˜ ì¤€ë¹„"):
                # ìƒíƒœ ì´ˆê¸°í™” í›„ ì¬ì‹¤í–‰í•˜ì—¬ ìŠ¤íŠ¸ë¦¼ ì¬ê°œ
                st.session_state.capture_ready = False
                st.session_state.captured_image_rgb = None
                st.session_state.capture_message = "ì¹´ë©”ë¼ë¥¼ ì¼œê³  ìì„¸ë¥¼ ì¡ì•„ì£¼ì„¸ìš”."
                st.rerun()

        else:
            st.warning("ì•„ì§ ìº¡ì²˜ëœ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. ì¡°ê±´ì„ ì¶©ì¡±ì‹œì¼œë³´ì„¸ìš”!")


if __name__ == "__main__":
    main()