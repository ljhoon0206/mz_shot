import time
import cv2
import numpy as np
import streamlit as st
import mediapipe as mp
import av
import queue
import base64
import streamlit.components.v1 as components

from streamlit_webrtc import (
    webrtc_streamer,
    VideoProcessorBase,
    RTCConfiguration,
    WebRtcMode,
)

mp_face = mp.solutions.face_detection


# ---------------- ì…”í„° ì†Œë¦¬ìš© HTML ìƒì„± í•¨ìˆ˜ ----------------
def load_shutter_html():
    """
    shutter.wav íŒŒì¼ì„ base64ë¡œ ì½ì–´ì„œ <audio> ìë™ ì¬ìƒ HTMLì„ ë§Œë“¤ì–´ ì¤Œ.
    shutter.wavëŠ” ì´ íŒŒì´ì¬ íŒŒì¼ê³¼ ê°™ì€ í´ë”ì— ìˆì–´ì•¼ í•¨.
    """
    try:
        # Note: 'shutter.wav' file must be present in the same directory.
        with open("shutter.wav", "rb") as f:
            audio_bytes = f.read()
        audio_b64 = base64.b64encode(audio_bytes).decode("utf-8")

        html = f"""
        <audio autoplay>
            <source src="data:audio/wav;base64,{audio_b64}" type="audio/wav" />
            ë¸Œë¼ìš°ì €ê°€ audio íƒœê·¸ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        </audio>
        """
        return html
    except FileNotFoundError:
        st.error("shutter.wav íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì…”í„° ì†Œë¦¬ ì¬ìƒì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")
        return ""


def get_face_roll_angle(img_bgr):
    """BGR ì´ë¯¸ì§€ì—ì„œ ì²« ë²ˆì§¸ ì–¼êµ´ì˜ roll angle(ê¸°ìš¸ê¸°) ê³„ì‚°."""
    h, w, _ = img_bgr.shape

    with mp_face.FaceDetection(
            model_selection=0,
            min_detection_confidence=0.6
    ) as face_detector:
        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
        results = face_detector.process(img_rgb)

        if not results.detections:
            return None, None

        detection = results.detections[0]
        keypoints = detection.location_data.relative_keypoints

        right_eye = keypoints[0]
        left_eye = keypoints[1]

        x1, y1 = right_eye.x * w, right_eye.y * h
        x2, y2 = left_eye.x * w, left_eye.y * h

        dx = x2 - x1
        dy = y2 - y1

        angle_rad = np.arctan2(dy, dx)
        angle_deg = np.degrees(angle_rad)

        right_eye_pt = (int(x1), int(y1))
        left_eye_pt = (int(x2), int(y2))

        return angle_deg, (right_eye_pt, left_eye_pt)


def draw_angle_overlay(img_bgr, angle_deg, eye_pts, label=""):
    """íƒ€ê²Ÿ ì´ë¯¸ì§€ ìœ„ì— ëˆˆ ì„  + ê°ë„ í…ìŠ¤íŠ¸ í‘œì‹œ."""
    img = img_bgr.copy()
    if angle_deg is not None and eye_pts is not None:
        (re, le) = eye_pts
        cv2.line(img, re, le, (0, 255, 0), 2)
        text = f"{label} roll: {angle_deg:.1f} deg"
    else:
        text = f"{label} No face"

    cv2.putText(
        img,
        text,
        (30, 40),
        cv2.FONT_HERSHEY_SIMPLEX,
        1.0,
        (0, 0, 255),
        2,
        cv2.LINE_AA,
    )
    return img


class PoseMatchProcessor(VideoProcessorBase):
    """
    ì›¹ìº  í”„ë ˆì„ì„ ë°›ì•„ì„œ:
      - ì–¼êµ´ roll angle ê³„ì‚° ë° ë¹„êµ
      - ì¡°ê±´ ë§Œì¡± ì‹œ ìë™ ìº¡ì²˜ (ë‹¨ì¼)
      - ìº¡ì²˜ ìˆœê°„ ì…”í„° ì†Œë¦¬ ì‹ í˜¸(shutter_queue) ë³´ëƒ„
    """

    def __init__(self):
        self._frame_format = "bgr24"

        self.ref_angle = None
        self.tolerance = 5.0
        self.capture_threshold = 90.0  # ìœ ì‚¬ë„ ìº¡ì²˜ ê¸°ì¤€ (90% ì´ìƒ)

        # ë‹¨ì¼ ìº¡ì²˜ ìƒíƒœ ê´€ë¦¬ ë³€ìˆ˜
        self.is_capturing_enabled = True  # ìº¡ì²˜ê°€ í™œì„±í™”ë˜ì—ˆëŠ”ì§€ (ì´ˆê¸°í™” ë²„íŠ¼ìœ¼ë¡œ ì œì–´)
        self.captured_image_rgb = None  # ìº¡ì²˜ëœ ë‹¨ì¼ ì´ë¯¸ì§€ (RGB)

        # Streamlit session_stateë¥¼ ì§ì ‘ ì¡°ì‘í•˜ì§€ ì•Šë„ë¡ ë³€ê²½
        # ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ ì£¼ê¸°ì ìœ¼ë¡œ ê°’ì„ ê°€ì ¸ê°€ë„ë¡ í•˜ê¸° ìœ„í•œ Queue
        self.capture_state_queue = queue.Queue(maxsize=1)
        self.shutter_queue = queue.Queue(maxsize=1)

        self.face_detector = mp_face.FaceDetection(
            model_selection=0,
            min_detection_confidence=0.6
        )

    def recv(self, frame):
        img_bgr = frame.to_ndarray(format="bgr24")
        raw_img_rgb = cv2.cvtColor(img_bgr.copy(), cv2.COLOR_BGR2RGB)  # ìº¡ì²˜ìš© RGB ì´ë¯¸ì§€

        h, w, _ = img_bgr.shape
        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
        results = self.face_detector.process(img_rgb)

        # ------------------- ì–¼êµ´ ê°ì§€ ë° ìœ ì‚¬ë„ ê³„ì‚° -------------------
        faces_for_capture = []
        max_sim = 0.0

        if results and results.detections:
            for det in results.detections:
                keypoints = det.location_data.relative_keypoints
                right_eye = keypoints[0]
                left_eye = keypoints[1]

                x1, y1 = right_eye.x * w, right_eye.y * h
                x2, y2 = left_eye.x * w, left_eye.y * h
                dx, dy = x2 - x1, y2 - y1

                angle_deg = np.degrees(np.arctan2(dy, dx))

                sim = None
                if self.ref_angle is not None:
                    diff = abs(angle_deg - self.ref_angle)
                    if diff < self.tolerance:
                        sim = max(0.0, 100.0 * (1.0 - diff / self.tolerance))

                if sim is not None:
                    faces_for_capture.append(sim)
                    max_sim = max(max_sim, sim)

            # ------------------- ìº¡ì²˜ ë¡œì§ (ë‹¨ì¼) -------------------

            # ì¡°ê±´: ìº¡ì²˜ í™œì„±í™” & íƒ€ê²Ÿ ì„¤ì •ë¨ & ìœ ì‚¬ë„ ê¸°ì¤€ ì¶©ì¡±
            if self.is_capturing_enabled and self.ref_angle is not None and max_sim >= self.capture_threshold:

                self.is_capturing_enabled = False  # ìº¡ì²˜ ë¹„í™œì„±í™”
                self.captured_image_rgb = raw_img_rgb  # ì´ë¯¸ì§€ ì €ì¥

                # ë©”ì¸ ìŠ¤ë ˆë“œì— ìº¡ì²˜ ì™„ë£Œ ì‹ í˜¸ ì „ì†¡
                try:
                    # ìº¡ì²˜ëœ RGB ì´ë¯¸ì§€ì™€ ì…”í„° ì‹ í˜¸ë¥¼ íì— ì „ì†¡
                    if self.capture_state_queue.empty():
                        self.capture_state_queue.put(self.captured_image_rgb.copy(), block=False)
                    if self.shutter_queue.empty():
                        self.shutter_queue.put(True, block=False)
                except queue.Full:
                    pass

                # í™”ë©´ì—ë§Œ CAPTURED! í…ìŠ¤íŠ¸ í‘œì‹œ
                cv2.putText(
                    img_bgr,
                    "CAPTURED!",
                    (30, h - 40),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    1.2,
                    (0, 0, 255),
                    3,
                    cv2.LINE_AA,
                )

            # ------------------- ì‹œê°ì  í”¼ë“œë°± (ìƒëµë˜ì§€ ì•ŠìŒ) -------------------
            # í˜„ì¬ ì–¼êµ´ ì •ë³´ í‘œì‹œ ë° ì˜¤ë²„ë ˆì´ ê·¸ë¦¬ê¸° (ì˜ˆ: ê°€ì¥ ë†’ì€ ìœ ì‚¬ë„ì˜ ì–¼êµ´ë§Œ)
            top_text = f"Faces: {len(results.detections)}"
            if self.ref_angle is not None:
                top_text += f" | target:{self.ref_angle:.1f} deg"
                if max_sim > 0.0:
                    top_text += f" | Max Sim: {max_sim:.0f}%"

        else:
            top_text = "No face"
            max_sim = 0.0

        # ìƒë‹¨ ê³µí†µ í…ìŠ¤íŠ¸
        cv2.putText(
            img_bgr,
            top_text,
            (30, 40),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.8,
            (0, 255, 255),
            2,
            cv2.LINE_AA,
        )

        return av.VideoFrame.from_ndarray(img_bgr, format="bgr24")


def main():
    st.title("íƒ€ê²Ÿ í¬ì¦ˆ ìœ ì‚¬ë„ ê¸°ë°˜ ë‹¨ì¼ ìë™ ì´¬ì˜")

    st.markdown(
        """
        **ê¸°ëŠ¥:** ì–¼êµ´ì˜ ë¡¤ ê°ë„(ê¸°ìš¸ê¸°)ë¥¼ íƒ€ê²Ÿ ì‚¬ì§„ê³¼ ë¹„êµí•˜ì—¬, ìœ ì‚¬ë„ê°€ **90% ì´ìƒ**ì¼ ë•Œ ìë™ìœ¼ë¡œ ë‹¨ì¼ ìº¡ì²˜ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        ---
        """
    )

    # ğŸ”” ì…”í„° ì†Œë¦¬ HTML ë¯¸ë¦¬ ì¤€ë¹„
    SHUTTER_HTML = load_shutter_html()

    # Session State ì´ˆê¸°í™” ë° ìº¡ì²˜ ìƒíƒœ ê´€ë¦¬
    if "capture_ready" not in st.session_state:
        st.session_state["capture_ready"] = False
        st.session_state["captured_image_rgb"] = None
        st.session_state["ref_angle"] = None
        st.session_state["tolerance"] = 8.0

    col1, col2 = st.columns([2, 1])

    with col2:
        st.header("ì„¤ì •")
        st.subheader("â‘  íƒ€ê²Ÿ ì‚¬ì§„ ì—…ë¡œë“œ")
        ref_file = st.file_uploader("íƒ€ê²Ÿ í¬ì¦ˆ ì‚¬ì§„ (jpg, png)", type=["jpg", "jpeg", "png"], key="ref_upload")

        ref_angle = None
        ref_disp = None

        if ref_file is not None:
            data = ref_file.read()
            arr = np.frombuffer(data, np.uint8)
            ref_img = cv2.imdecode(arr, cv2.IMREAD_COLOR)

            if ref_img is None:
                st.error("íƒ€ê²Ÿ ì‚¬ì§„ì„ ì½ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            else:
                ref_angle_new, eye_pts = get_face_roll_angle(ref_img)
                if ref_angle_new is None:
                    st.error("íƒ€ê²Ÿ ì‚¬ì§„ì—ì„œ ì–¼êµ´ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                else:
                    ref_angle = ref_angle_new
                    st.success(f"íƒ€ê²Ÿ ì–¼êµ´ ê°ë„: {ref_angle:.1f}Â°")
                    ref_disp = draw_angle_overlay(ref_img, ref_angle, eye_pts, label="target")

        st.session_state["ref_angle"] = ref_angle

        st.subheader("â‘¡ ì´¬ì˜ ì¡°ê±´")
        tolerance = st.slider("í—ˆìš© ê°ë„ ì°¨ (deg)", min_value=2.0, max_value=30.0, value=st.session_state["tolerance"],
                              step=1.0)
        st.session_state["tolerance"] = tolerance

        st.markdown("---")

        if ref_disp is not None:
            st.subheader("íƒ€ê²Ÿ í¬ì¦ˆ")
            st.image(ref_disp, channels="BGR", use_container_width=True)

    with col1:
        st.subheader("ì›¹ìº  ìŠ¤íŠ¸ë¦¼")
        if st.session_state["capture_ready"]:
            st.info("âœ… ìº¡ì²˜ ì™„ë£Œ! ì´ˆê¸°í™” ë²„íŠ¼ì„ ëˆŒëŸ¬ ë‹¤ì‹œ ì´¬ì˜í•˜ì„¸ìš”.")

        rtc_config = RTCConfiguration(
            {"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
        )

        cam_mode = st.radio("ì¹´ë©”ë¼ ì„ íƒ", ["ì „ë©´", "í›„ë©´"], horizontal=True, key="cam_mode",
                            disabled=st.session_state["capture_ready"])

        base_constraints = {
            "width": {"ideal": 480},
            "height": {"ideal": 360},
            "frameRate": {"ideal": 15},
        }

        video_constraints = {
            **base_constraints,
            "facingMode": {"ideal": "user"} if cam_mode == "ì „ë©´" else {"ideal": "environment"},
        }

        # ìº¡ì²˜ ì™„ë£Œ ìƒíƒœì´ë©´ ì›¹ìº ì„ ë¹„í™œì„±í™” (None)
        if not st.session_state["capture_ready"]:
            webrtc_ctx = webrtc_streamer(
                key="pose-match-capture-single",
                mode=WebRtcMode.SENDRECV,
                rtc_configuration=rtc_config,
                media_stream_constraints={"video": video_constraints, "audio": False},
                video_processor_factory=PoseMatchProcessor,
                async_processing=True,
            )
        else:
            # ìº¡ì²˜ ì™„ë£Œ ì‹œ ì›¹ìº  ëŒ€ì‹  ë¹ˆ ì»¨í…Œì´ë„ˆ í‘œì‹œ
            st.empty()
            webrtc_ctx = None

    # ------------------ ë©”ì¸ ìŠ¤ë ˆë“œ ìº¡ì²˜ ìƒíƒœ ì—…ë°ì´íŠ¸ ë£¨í”„ (WebRTC ì»¨í…ìŠ¤íŠ¸ ì´ìš©) ------------------
    if webrtc_ctx and webrtc_ctx.video_processor:
        vp: PoseMatchProcessor = webrtc_ctx.video_processor

        # íƒ€ê²Ÿ ê°ë„/ì„¤ì • ì „ë‹¬
        vp.ref_angle = st.session_state["ref_angle"]
        vp.tolerance = st.session_state["tolerance"]

        # í”„ë¡œì„¸ì„œ ìƒíƒœ í™•ì¸
        try:
            # â­ ìº¡ì²˜ ì™„ë£Œ ì‹ í˜¸ í™•ì¸ (ë…¼ë¸”ë¡œí‚¹)
            captured_img = vp.capture_state_queue.get(timeout=0.01)

            if captured_img is not None:
                # ìº¡ì²˜ëœ ì´ë¯¸ì§€ì™€ ìƒíƒœë¥¼ Session Stateì— ì €ì¥
                st.session_state["captured_image_rgb"] = captured_img
                st.session_state["capture_ready"] = True

                # ì…”í„° ì†Œë¦¬ ì¬ìƒ
                components.html(SHUTTER_HTML, height=0)

                # UI ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ Streamlit ì¬ì‹¤í–‰
                st.rerun()
        except queue.Empty:
            pass  # ì‹ í˜¸ ì—†ìœ¼ë©´ ê³„ì† ì§„í–‰
        except Exception:
            pass

    # ------------------ ìº¡ì²˜ ì™„ë£Œ í›„ UI ë¡œì§ (Streamlit Standard UI) ------------------
    with col1:
        st.markdown("---")
        st.subheader("âœ¨ ìµœì¢… ìº¡ì²˜ ê²°ê³¼")

        if st.session_state["capture_ready"] and st.session_state["captured_image_rgb"] is not None:
            st.success("ğŸ‰ **ë‹¨ì¼ ìº¡ì²˜ ì™„ë£Œ!**")

            captured_img_rgb = st.session_state["captured_image_rgb"]

            st.image(captured_img_rgb, caption="ìµœì¢… ìº¡ì²˜ ì´ë¯¸ì§€", use_container_width=True)

            # ë‹¤ìš´ë¡œë“œë¥¼ ìœ„í•´ RGBë¥¼ BGRë¡œ ë³€í™˜ í›„ PNG ì¸ì½”ë”©
            img_bgr = cv2.cvtColor(captured_img_rgb, cv2.COLOR_RGB2BGR)
            ret, buffer = cv2.imencode(".png", img_bgr)

            if ret:
                st.download_button(
                    label="ğŸ–¼ï¸ ìº¡ì²˜ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ",
                    data=buffer.tobytes(),
                    file_name=f"pose_capture_{int(time.time())}.png",
                    mime="image/png"
                )

            if st.button("ğŸ”„ ë‹¤ìŒ ìº¡ì²˜ ì¤€ë¹„"):
                st.session_state["capture_ready"] = False
                st.session_state["captured_image_rgb"] = None

                # VideoProcessorì˜ ìƒíƒœ ì´ˆê¸°í™” ìš”ì²­ (ë©”ì¸ ìŠ¤ë ˆë“œê°€ ë‹¤ìŒ ì‹¤í–‰ ì‹œ ì²˜ë¦¬)
                if webrtc_ctx and webrtc_ctx.video_processor:
                    vp: PoseMatchProcessor = webrtc_ctx.video_processor
                    vp.is_capturing_enabled = True
                    vp.captured_image_rgb = None
                    # í ë¹„ìš°ê¸° (ë¶ˆí•„ìš”í•œ ì‹ í˜¸ ë°©ì§€)
                    while not vp.capture_state_queue.empty():
                        vp.capture_state_queue.get_nowait()
                    while not vp.shutter_queue.empty():
                        vp.shutter_queue.get_nowait()

                st.rerun()

        elif st.session_state["ref_angle"] is None:
            st.warning("ë¨¼ì € íƒ€ê²Ÿ ì‚¬ì§„ì„ ì—…ë¡œë“œí•˜ì—¬ íƒ€ê²Ÿ ê°ë„ë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
        else:
            st.info("ì›¹ìº ì„ ì¼œê³ , ì–¼êµ´ ê°ë„ë¥¼ íƒ€ê²Ÿê³¼ ìœ ì‚¬í•˜ê²Œ ë§ì¶°ë³´ì„¸ìš”. (ìœ ì‚¬ë„ 90% ì´ìƒ)")


if __name__ == "__main__":
    main()